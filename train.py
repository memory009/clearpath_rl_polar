#!/usr/bin/env python3
"""
Clearpath TD3+POLAR è®­ç»ƒè„šæœ¬
"""

import sys
import os
import numpy as np
import time
from datetime import datetime
from tqdm import tqdm

# æ·»åŠ è·¯å¾„
sys.path.append(os.path.dirname(__file__))

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from algorithms.td3_polar import TD3Agent
from envs.clearpath_nav_env import ClearpathNavEnv
from utils.config import TD3Config


def train_td3_polar(config=None):
    """
    ä¸»è®­ç»ƒå‡½æ•°
    
    Args:
        config: TD3Configå¯¹è±¡
    """
    if config is None:
        config = TD3Config()
    
    print("\n" + "="*80)
    print("Clearpath TD3+POLAR è®­ç»ƒ")
    print("="*80)
    
    # 1. åˆ›å»ºç¯å¢ƒ
    print("\n[1/4] åˆ›å»ºç¯å¢ƒ...")
    env = ClearpathNavEnv(
        robot_name=config.robot_name,
        goal_pos=config.goal_pos,
        max_steps=config.max_steps,
        collision_threshold=config.collision_threshold
    )
    print(f"âœ“ ç¯å¢ƒOK: è§‚æµ‹={env.observation_space.shape}, åŠ¨ä½œ={env.action_space.shape}")
    
    # 2. åˆ›å»ºæ™ºèƒ½ä½“
    print("\n[2/4] åˆ›å»ºTD3æ™ºèƒ½ä½“...")
    agent = TD3Agent(
        state_dim=config.state_dim,
        action_dim=config.action_dim,
        max_action=config.max_action,
        config=config
    )
    
    # 3. åˆ›å»ºæ—¥å¿—ç›®å½•
    print("\n[3/4] å‡†å¤‡æ—¥å¿—...")
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = os.path.join(config.log_path, timestamp)
    os.makedirs(log_dir, exist_ok=True)
    print(f"âœ“ æ—¥å¿—ç›®å½•: {log_dir}")
    
    # 4. è®­ç»ƒå¾ªç¯
    print("\n[4/4] å¼€å§‹è®­ç»ƒ...")
    print(f"  æ€»æ­¥æ•°: {config.total_timesteps}")
    print(f"  éšæœºæ¢ç´¢: {config.start_timesteps}æ­¥")
    print(f"  POLARéªŒè¯é—´éš”: {config.verify_interval}æ­¥")
    
    state, _ = env.reset()
    episode_reward = 0
    episode_steps = 0
    episode_num = 0
    
    # ç»Ÿè®¡å˜é‡
    rewards_history = []
    safe_count = 0
    total_verify = 0
    
    pbar = tqdm(total=config.total_timesteps, desc="è®­ç»ƒè¿›åº¦")
    
    for t in range(config.total_timesteps):
        # é€‰æ‹©åŠ¨ä½œ
        if t < config.start_timesteps:
            action = env.action_space.sample()
        else:
            action = agent.select_action(state)
            noise = np.random.normal(0, config.max_action * config.expl_noise, 
                                    size=config.action_dim)
            action = (action + noise).clip(-config.max_action, config.max_action)
        
        # æ‰§è¡ŒåŠ¨ä½œ
        next_state, reward, done, truncated, info = env.step(action)
        done_bool = float(done) if episode_steps < config.max_steps else 0
        
        # å­˜å‚¨ç»éªŒ
        agent.store_transition(state, action, next_state, reward, done_bool)
        
        state = next_state
        episode_reward += reward
        episode_steps += 1
        
        # æ›´æ–°ç½‘ç»œ
        if t >= config.start_timesteps:
            agent.train()
        
        # POLARéªŒè¯
        if t > 0 and t % config.verify_interval == 0:
            try:
                is_safe, action_ranges = agent.verify_safety(
                    state,
                    observation_error=config.observation_error,
                    bern_order=config.bern_order,
                    error_steps=config.error_steps
                )
                
                total_verify += 1
                if is_safe:
                    safe_count += 1
                
                pbar.write(f"[éªŒè¯] æ­¥æ•°{t}: å®‰å…¨ç‡={safe_count}/{total_verify} ({safe_count/total_verify:.1%})")
            except Exception as e:
                pbar.write(f"[è­¦å‘Š] POLARéªŒè¯å¤±è´¥: {e}")
        
        # Episodeç»“æŸ
        if done or truncated:
            rewards_history.append(episode_reward)
            
            msg = f"Episode {episode_num}: R={episode_reward:.1f}, Steps={episode_steps}"
            if info.get('goal_reached'):
                msg += " ğŸ¯æˆåŠŸ"
            elif info.get('collision'):
                msg += " âš ï¸ç¢°æ’"
            pbar.write(msg)
            
            # é‡ç½®
            state, _ = env.reset()
            episode_reward = 0
            episode_steps = 0
            episode_num += 1
            
            # å®šæœŸä¿å­˜
            if episode_num % 50 == 0:
                model_path = os.path.join(config.model_path, timestamp)
                os.makedirs(model_path, exist_ok=True)
                agent.save(f"{model_path}/episode_{episode_num}")
                pbar.write(f"âœ“ æ¨¡å‹å·²ä¿å­˜: episode_{episode_num}")
        
        pbar.update(1)
        pbar.set_postfix({
            'Ep': episode_num,
            'R': f'{episode_reward:.0f}',
            'Safe': f'{safe_count}/{total_verify}'
        })
    
    pbar.close()
    
    # è®­ç»ƒå®Œæˆ
    print("\n" + "="*80)
    print("è®­ç»ƒå®Œæˆï¼")
    print("="*80)
    print(f"æ€»Episode: {episode_num}")
    print(f"å¹³å‡å¥–åŠ±: {np.mean(rewards_history):.2f}")
    print(f"æœ€é«˜å¥–åŠ±: {np.max(rewards_history):.2f}")
    if total_verify > 0:
        print(f"å®‰å…¨ç‡: {safe_count/total_verify:.2%}")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_path = os.path.join(config.model_path, f"final_{timestamp}")
    os.makedirs(final_path, exist_ok=True)
    agent.save(final_path)
    agent.save_weights(f"{final_path}/weights_for_polar.npz")
    print(f"\nâœ“ æœ€ç»ˆæ¨¡å‹: {final_path}")
    print(f"âœ“ POLARæƒé‡: {final_path}/weights_for_polar.npz")
    
    env.close()
    
    return agent


if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser()
    parser.add_argument('--timesteps', type=int, default=100000)
    parser.add_argument('--start_timesteps', type=int, default=25000)
    parser.add_argument('--verify_interval', type=int, default=1000)
    
    args = parser.parse_args()
    
    config = TD3Config()
    config.total_timesteps = args.timesteps
    config.start_timesteps = args.start_timesteps
    config.verify_interval = args.verify_interval
    
    agent = train_td3_polar(config)
    
    print("\nâœ“ å…¨éƒ¨å®Œæˆï¼")
