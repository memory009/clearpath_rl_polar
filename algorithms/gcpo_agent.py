#!/usr/bin/env python3
"""
GCPO (Goal-Conditioned On-Policy) Agent
åŸºäºNeurIPS 2024è®ºæ–‡çš„å®ç°
"""

import torch
import numpy as np
import copy
from collections import deque

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(__file__)))

from algorithms.td3_polar import TD3Agent
from algorithms.networks import Actor, Critic


class CapabilityEstimator:
    """
    èƒ½åŠ›ä¼°è®¡å™¨ - GCPOæ ¸å¿ƒç»„ä»¶
    ä¼°è®¡æ™ºèƒ½ä½“è¾¾åˆ°ä¸åŒç›®æ ‡çš„èƒ½åŠ›
    """
    
    def __init__(self, window_size=100):
        """
        Args:
            window_size: æ»‘åŠ¨çª—å£å¤§å°
        """
        self.window_size = window_size
        
        # å­˜å‚¨æ¯ä¸ªç›®æ ‡çš„æˆåŠŸå†å²
        # key: (goal_x, goal_y), value: deque of success/failure
        self.goal_history = {}
        
        # è·ç¦»åŒºé—´ç»Ÿè®¡
        self.distance_bins = [0.5, 1.0, 1.5, 2.0, 3.0, 4.0, 5.0, 10.0]
        self.distance_stats = {d: deque(maxlen=50) for d in self.distance_bins}
    
    def update(self, goal, success):
        """
        æ›´æ–°ç›®æ ‡çš„æˆåŠŸå†å²
        
        Args:
            goal: ç›®æ ‡ä½ç½® (x, y)
            success: æ˜¯å¦æˆåŠŸ (bool)
        """
        goal_key = self._discretize_goal(goal)
        
        if goal_key not in self.goal_history:
            self.goal_history[goal_key] = deque(maxlen=self.window_size)
        
        self.goal_history[goal_key].append(1 if success else 0)
        
        # æ›´æ–°è·ç¦»ç»Ÿè®¡
        distance = np.linalg.norm(goal)
        bin_idx = self._get_distance_bin(distance)
        if bin_idx is not None:
            self.distance_stats[self.distance_bins[bin_idx]].append(1 if success else 0)
    
    def estimate_capability(self, goal):
        """
        ä¼°è®¡è¾¾åˆ°ç›®æ ‡çš„èƒ½åŠ› (æˆåŠŸæ¦‚ç‡)
        
        Args:
            goal: ç›®æ ‡ä½ç½® (x, y)
        
        Returns:
            capability: ä¼°è®¡çš„æˆåŠŸæ¦‚ç‡ [0, 1]
        """
        goal_key = self._discretize_goal(goal)
        
        # 1. å¦‚æœæœ‰è¯¥ç›®æ ‡çš„å†å²è®°å½•
        if goal_key in self.goal_history and len(self.goal_history[goal_key]) > 5:
            return np.mean(self.goal_history[goal_key])
        
        # 2. å¦åˆ™ä½¿ç”¨è·ç¦»binçš„ç»Ÿè®¡
        distance = np.linalg.norm(goal)
        bin_idx = self._get_distance_bin(distance)
        
        if bin_idx is not None:
            bin_key = self.distance_bins[bin_idx]
            if len(self.distance_stats[bin_key]) > 3:
                return np.mean(self.distance_stats[bin_key])
        
        # 3. é»˜è®¤å€¼ï¼šæ ¹æ®è·ç¦»ç»™ä¸€ä¸ªå…ˆéªŒ
        # è¿‘è·ç¦»ç›®æ ‡æ›´å®¹æ˜“
        if distance < 1.0:
            return 0.7
        elif distance < 2.0:
            return 0.5
        elif distance < 3.0:
            return 0.3
        else:
            return 0.1
    
    def _discretize_goal(self, goal, resolution=0.5):
        """å°†ç›®æ ‡ç¦»æ•£åŒ–åˆ°ç½‘æ ¼"""
        x = round(goal[0] / resolution) * resolution
        y = round(goal[1] / resolution) * resolution
        return (x, y)
    
    def _get_distance_bin(self, distance):
        """è·å–è·ç¦»æ‰€åœ¨çš„binç´¢å¼•"""
        for i, bin_max in enumerate(self.distance_bins):
            if distance < bin_max:
                return i
        return None
    
    def get_statistics(self):
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'num_goals_tracked': len(self.goal_history),
            'distance_bin_stats': {}
        }
        
        for bin_max, history in self.distance_stats.items():
            if len(history) > 0:
                stats['distance_bin_stats'][bin_max] = {
                    'samples': len(history),
                    'success_rate': np.mean(history)
                }
        
        return stats


class SelfCurriculum:
    """
    è‡ªé€‚åº”è¯¾ç¨‹ç”Ÿæˆå™¨ - GCPOæ ¸å¿ƒç»„ä»¶
    æ ¹æ®èƒ½åŠ›ä¼°è®¡è‡ªåŠ¨é€‰æ‹©åˆé€‚éš¾åº¦çš„ç›®æ ‡
    """
    
    def __init__(self, goal_space_bounds, capability_estimator):
        """
        Args:
            goal_space_bounds: ç›®æ ‡ç©ºé—´è¾¹ç•Œ [(x_min, x_max), (y_min, y_max)]
            capability_estimator: èƒ½åŠ›ä¼°è®¡å™¨
        """
        self.bounds = goal_space_bounds
        self.capability_estimator = capability_estimator
        
        # è¯¾ç¨‹å‚æ•°
        self.target_capability = 0.5  # ç›®æ ‡æˆåŠŸç‡ (å¤ªç®€å•æˆ–å¤ªéš¾éƒ½ä¸å¥½)
        self.capability_tolerance = 0.2  # å®¹å¿èŒƒå›´
        
        # é‡‡æ ·ç­–ç•¥
        self.exploration_prob = 0.2  # æ¢ç´¢æ¦‚ç‡
    
    def sample_goal(self):
        """
        é‡‡æ ·ä¸€ä¸ªåˆé€‚éš¾åº¦çš„ç›®æ ‡
        
        Returns:
            goal: (x, y)
        """
        # ä»¥ä¸€å®šæ¦‚ç‡è¿›è¡Œæ¢ç´¢(éšæœºé‡‡æ ·)
        if np.random.random() < self.exploration_prob:
            return self._random_goal()
        
        # å¦åˆ™é‡‡æ ·"æ°å¥½åˆé€‚"çš„ç›®æ ‡
        max_attempts = 50
        best_goal = None
        best_score = -float('inf')
        
        for _ in range(max_attempts):
            candidate = self._random_goal()
            capability = self.capability_estimator.estimate_capability(candidate)
            
            # è¯„åˆ†ï¼šè¶Šæ¥è¿‘target_capabilityè¶Šå¥½
            score = -abs(capability - self.target_capability)
            
            if score > best_score:
                best_score = score
                best_goal = candidate
        
        return best_goal
    
    def _random_goal(self):
        """åœ¨ç›®æ ‡ç©ºé—´ä¸­éšæœºé‡‡æ ·"""
        x = np.random.uniform(self.bounds[0][0], self.bounds[0][1])
        y = np.random.uniform(self.bounds[1][0], self.bounds[1][1])
        return np.array([x, y], dtype=np.float32)


class GCPOAgent(TD3Agent):
    """
    GCPOæ™ºèƒ½ä½“
    ç»§æ‰¿TD3Agent,æ·»åŠ èƒ½åŠ›ä¼°è®¡å’Œè‡ªé€‚åº”è¯¾ç¨‹
    """
    
    def __init__(self, state_dim, action_dim, max_action, config):
        """
        åˆå§‹åŒ–GCPOæ™ºèƒ½ä½“
        
        Args:
            state_dim: çŠ¶æ€ç»´åº¦
            action_dim: åŠ¨ä½œç»´åº¦
            max_action: æœ€å¤§åŠ¨ä½œå€¼
            config: é…ç½®å¯¹è±¡
        """
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        super().__init__(state_dim, action_dim, max_action, config)
        
        # GCPOç‰¹æœ‰ç»„ä»¶
        self.capability_estimator = CapabilityEstimator(window_size=100)
        
        # ç›®æ ‡ç©ºé—´å®šä¹‰
        goal_space_bounds = getattr(config, 'goal_space_bounds', 
                                    [(0.5, 5.5), (-3.0, 3.5)])
        
        self.curriculum = SelfCurriculum(
            goal_space_bounds,
            self.capability_estimator
        )
        
        # è®­ç»ƒæ¨¡å¼
        self.training_mode = 'bc'  # 'bc' or 'rl'
        
        print(f"âœ“ GCPO Agentåˆå§‹åŒ–å®Œæˆ")
        print(f"  èƒ½åŠ›ä¼°è®¡: æ»‘åŠ¨çª—å£={self.capability_estimator.window_size}")
        print(f"  ç›®æ ‡ç©ºé—´: x={goal_space_bounds[0]}, y={goal_space_bounds[1]}")
    
    def update_capability(self, goal, success):
        """
        æ›´æ–°èƒ½åŠ›ä¼°è®¡
        
        Args:
            goal: ç›®æ ‡ä½ç½® (x, y)
            success: æ˜¯å¦æˆåŠŸ
        """
        self.capability_estimator.update(goal, success)
    
    def sample_curriculum_goal(self):
        """
        ä»è¯¾ç¨‹ä¸­é‡‡æ ·ç›®æ ‡
        
        Returns:
            goal: (x, y)
        """
        return self.curriculum.sample_goal()
    
    def switch_to_rl_mode(self):
        """åˆ‡æ¢åˆ°RLè®­ç»ƒæ¨¡å¼"""
        self.training_mode = 'rl'
        print("\n" + "="*60)
        print("ğŸ”„ åˆ‡æ¢åˆ°RL Fine-tuningæ¨¡å¼")
        print("="*60)
        print("  BCé¢„è®­ç»ƒ â†’ RLè‡ªé€‚åº”è¯¾ç¨‹")
        print("="*60 + "\n")
    
    def get_capability_stats(self):
        """è·å–èƒ½åŠ›ä¼°è®¡ç»Ÿè®¡"""
        return self.capability_estimator.get_statistics()
    
    def save_gcpo(self, path):
        """
        ä¿å­˜GCPOç‰¹æœ‰çš„ç»„ä»¶
        
        Args:
            path: ä¿å­˜è·¯å¾„
        """
        # ä¿å­˜åŸºç¡€æ¨¡å‹
        self.save(path)
        
        # ä¿å­˜èƒ½åŠ›ä¼°è®¡å™¨å†å²
        import pickle
        
        capability_data = {
            'goal_history': dict(self.capability_estimator.goal_history),
            'distance_stats': dict(self.capability_estimator.distance_stats)
        }
        
        with open(os.path.join(path, 'capability_estimator.pkl'), 'wb') as f:
            pickle.dump(capability_data, f)
        
        print(f"âœ“ GCPOç»„ä»¶å·²ä¿å­˜åˆ°: {path}")
    
    def load_gcpo(self, path):
        """
        åŠ è½½GCPOç‰¹æœ‰çš„ç»„ä»¶
        
        Args:
            path: åŠ è½½è·¯å¾„
        """
        # åŠ è½½åŸºç¡€æ¨¡å‹
        self.load(path)
        
        # åŠ è½½èƒ½åŠ›ä¼°è®¡å™¨
        import pickle
        
        capability_path = os.path.join(path, 'capability_estimator.pkl')
        if os.path.exists(capability_path):
            with open(capability_path, 'rb') as f:
                capability_data = pickle.load(f)
            
            # è½¬æ¢å›deque
            for key, value in capability_data['goal_history'].items():
                self.capability_estimator.goal_history[key] = deque(
                    value, maxlen=self.capability_estimator.window_size
                )
            
            for key, value in capability_data['distance_stats'].items():
                self.capability_estimator.distance_stats[key] = deque(
                    value, maxlen=50
                )
            
            print(f"âœ“ GCPOç»„ä»¶å·²åŠ è½½: {path}")


# ==================== æµ‹è¯•ä»£ç  ====================
if __name__ == '__main__':
    print("="*60)
    print("GCPO Agent æµ‹è¯•")
    print("="*60)
    
    from utils.config import TD3Config
    
    # 1. åˆ›å»ºGCPOæ™ºèƒ½ä½“
    print("\n[1/3] åˆ›å»ºGCPOæ™ºèƒ½ä½“...")
    config = TD3Config()
    agent = GCPOAgent(
        state_dim=12,
        action_dim=2,
        max_action=0.5,
        config=config
    )
    print("âœ“ æ™ºèƒ½ä½“åˆ›å»ºæˆåŠŸ")
    
    # 2. æµ‹è¯•èƒ½åŠ›ä¼°è®¡
    print("\n[2/3] æµ‹è¯•èƒ½åŠ›ä¼°è®¡...")
    test_goals = [
        (1.0, 1.0),
        (2.0, 2.0),
        (3.0, 1.5),
        (5.0, -2.0)
    ]
    
    # æ¨¡æ‹Ÿä¸€äº›æˆåŠŸ/å¤±è´¥è®°å½•
    for goal in test_goals:
        for _ in range(20):
            # è¿‘è·ç¦»ç›®æ ‡æˆåŠŸç‡é«˜
            distance = np.linalg.norm(goal)
            success = np.random.random() < (1.0 / (1.0 + distance * 0.3))
            agent.update_capability(goal, success)
    
    print("  å„ç›®æ ‡çš„èƒ½åŠ›ä¼°è®¡:")
    for goal in test_goals:
        capability = agent.capability_estimator.estimate_capability(goal)
        print(f"    {goal}: {capability:.2%}")
    
    # 3. æµ‹è¯•è¯¾ç¨‹é‡‡æ ·
    print("\n[3/3] æµ‹è¯•è¯¾ç¨‹é‡‡æ ·...")
    print("  é‡‡æ ·10ä¸ªè¯¾ç¨‹ç›®æ ‡:")
    for i in range(10):
        goal = agent.sample_curriculum_goal()
        capability = agent.capability_estimator.estimate_capability(goal)
        print(f"    {i+1}. ({goal[0]:.2f}, {goal[1]:.2f}) - ä¼°è®¡èƒ½åŠ›: {capability:.2%}")
    
    # 4. æµ‹è¯•ä¿å­˜å’ŒåŠ è½½
    print("\n[4/4] æµ‹è¯•ä¿å­˜å’ŒåŠ è½½...")
    test_path = "./test_gcpo_model"
    agent.save_gcpo(test_path)
    print("âœ“ æ¨¡å‹å·²ä¿å­˜")
    
    # åˆ›å»ºæ–°æ™ºèƒ½ä½“å¹¶åŠ è½½
    agent2 = GCPOAgent(12, 2, 0.5, config)
    agent2.load_gcpo(test_path)
    print("âœ“ æ¨¡å‹å·²åŠ è½½")
    
    # æ¸…ç†æµ‹è¯•æ–‡ä»¶
    import shutil
    if os.path.exists(test_path):
        shutil.rmtree(test_path)
        print("âœ“ æµ‹è¯•æ–‡ä»¶å·²æ¸…ç†")
    
    print("\n" + "="*60)
    print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
    print("="*60)