#!/usr/bin/env python3
"""
GCPO å®Œæ•´è®­ç»ƒæµç¨‹
é˜¶æ®µ1: BCé¢„è®­ç»ƒ â†’ é˜¶æ®µ2: RL Fine-tuning (è‡ªé€‚åº”è¯¾ç¨‹)
"""

import sys
import os
import numpy as np
from datetime import datetime
from tqdm import tqdm

sys.path.append(os.path.dirname(__file__))

from algorithms.gcpo_agent import GCPOAgent
from envs.clearpath_nav_env import ClearpathNavEnv
from utils.config import TD3Config


class GCPOConfig(TD3Config):
    """GCPOè®­ç»ƒé…ç½®"""
    
    # BCé¢„è®­ç»ƒå‚æ•°
    bc_epochs = 100
    bc_batch_size = 256
    bc_learning_rate = 3e-4
    bc_val_split = 0.1
    
    # RL Fine-tuningå‚æ•°
    rl_total_timesteps = 100000
    rl_start_timesteps = 5000  # æ¯”åŸæ¥çŸ­,å› ä¸ºæœ‰BCåˆå§‹åŒ–
    
    # è¯¾ç¨‹å­¦ä¹ å‚æ•°
    goal_space_bounds = [(0.5, 5.5), (-3.0, 3.5)]
    capability_window = 100
    target_capability = 0.5  # ç›®æ ‡æˆåŠŸç‡
    
    # èƒ½åŠ›è¯„ä¼°
    eval_interval = 5000  # æ¯éš”å¤šå°‘æ­¥è¯„ä¼°ä¸€æ¬¡
    eval_episodes = 10


def train_gcpo_rl_phase(
    agent, 
    config, 
    start_from_bc=True,
    save_dir='./models/gcpo'
):
    """
    GCPO RL Fine-tuningé˜¶æ®µ
    
    Args:
        agent: é¢„è®­ç»ƒçš„GCPOæ™ºèƒ½ä½“
        config: é…ç½®
        start_from_bc: æ˜¯å¦ä»BCé¢„è®­ç»ƒå¼€å§‹
        save_dir: ä¿å­˜ç›®å½•
    
    Returns:
        agent: è®­ç»ƒåçš„æ™ºèƒ½ä½“
    """
    print("\n" + "="*80)
    print("ğŸš€ GCPO RL Fine-tuning")
    print("="*80)
    
    if start_from_bc:
        print("  èµ·ç‚¹: BCé¢„è®­ç»ƒç­–ç•¥")
    else:
        print("  èµ·ç‚¹: éšæœºç­–ç•¥")
    
    print(f"  æ€»æ­¥æ•°: {config.rl_total_timesteps}")
    print(f"  åˆå§‹æ¢ç´¢: {config.rl_start_timesteps}æ­¥")
    print(f"  è‡ªé€‚åº”è¯¾ç¨‹: âœ“ å¯ç”¨")
    print("="*80 + "\n")
    
    # åˆ‡æ¢åˆ°RLæ¨¡å¼
    agent.switch_to_rl_mode()
    
    # åˆ›å»ºæ—¥å¿—
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = os.path.join('./logs', f'gcpo_rl_{timestamp}')
    model_dir = os.path.join(save_dir, f'gcpo_rl_{timestamp}')
    os.makedirs(log_dir, exist_ok=True)
    os.makedirs(model_dir, exist_ok=True)
    
    # è®­ç»ƒç»Ÿè®¡
    total_timesteps = 0
    episode_num = 0
    rewards_history = []
    success_history = []
    goal_history = []
    
    # POLARéªŒè¯ç»Ÿè®¡
    safe_count = 0
    total_verify = 0
    
    # åˆ›å»ºè¿›åº¦æ¡
    pbar = tqdm(total=config.rl_total_timesteps, desc="RLè®­ç»ƒ")
    
    # ä¸»è®­ç»ƒå¾ªç¯
    while total_timesteps < config.rl_total_timesteps:
        # 1. ä»è¯¾ç¨‹ä¸­é‡‡æ ·ç›®æ ‡
        curriculum_goal = agent.sample_curriculum_goal()
        
        # 2. åˆ›å»ºç¯å¢ƒ(ä½¿ç”¨è¯¾ç¨‹ç›®æ ‡)
        env = ClearpathNavEnv(goal_pos=tuple(curriculum_goal))
        
        # 3. é‡ç½®ç¯å¢ƒ
        state, _ = env.reset()
        episode_reward = 0
        episode_steps = 0
        
        # Episodeå¾ªç¯
        for step in range(config.max_steps):
            # é€‰æ‹©åŠ¨ä½œ
            if total_timesteps < config.rl_start_timesteps:
                # åˆå§‹æ¢ç´¢é˜¶æ®µ:éšæœºåŠ¨ä½œ
                action = env.action_space.sample()
            else:
                # ä½¿ç”¨ç­–ç•¥
                action = agent.select_action(state)
                # æ·»åŠ æ¢ç´¢å™ªå£°
                noise = np.random.normal(0, config.max_action * config.expl_noise, 
                                        size=config.action_dim)
                action = (action + noise).clip(-config.max_action, config.max_action)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, done, truncated, info = env.step(action)
            done_bool = float(done) if episode_steps < config.max_steps else 0
            
            # å­˜å‚¨ç»éªŒ
            agent.store_transition(state, action, next_state, reward, done_bool)
            
            state = next_state
            episode_reward += reward
            episode_steps += 1
            total_timesteps += 1
            
            # è®­ç»ƒç½‘ç»œ
            if total_timesteps >= config.rl_start_timesteps:
                agent.train()
            
            # POLARéªŒè¯(å¯é€‰)
            if total_timesteps > 0 and total_timesteps % config.verify_interval == 0:
                try:
                    is_safe, _ = agent.verify_safety(
                        state,
                        observation_error=config.observation_error,
                        bern_order=config.bern_order,
                        error_steps=config.error_steps
                    )
                    total_verify += 1
                    if is_safe:
                        safe_count += 1
                except:
                    pass
            
            pbar.update(1)
            
            if done or truncated:
                break
        
        # Episodeç»“æŸå¤„ç†
        episode_success = info.get('goal_reached', False)
        
        # æ›´æ–°èƒ½åŠ›ä¼°è®¡
        agent.update_capability(curriculum_goal, episode_success)
        
        # è®°å½•ç»Ÿè®¡
        rewards_history.append(episode_reward)
        success_history.append(episode_success)
        goal_history.append(curriculum_goal.copy())
        
        # æ‰“å°ä¿¡æ¯
        capability = agent.capability_estimator.estimate_capability(curriculum_goal)
        msg = (f"Ep {episode_num}: "
               f"Goal=({curriculum_goal[0]:.1f},{curriculum_goal[1]:.1f}), "
               f"R={episode_reward:.1f}, "
               f"Steps={episode_steps}, "
               f"Cap={capability:.1%}")
        
        if episode_success:
            msg += " ğŸ¯"
        elif info.get('collision'):
            msg += " âš ï¸"
        
        pbar.write(msg)
        
        episode_num += 1
        
        # å®šæœŸç»Ÿè®¡å’Œä¿å­˜
        if episode_num % 50 == 0:
            recent_success = np.mean(success_history[-50:])
            avg_reward = np.mean(rewards_history[-50:])
            
            pbar.write("\n" + "="*60)
            pbar.write(f"Episode {episode_num} ç»Ÿè®¡:")
            pbar.write(f"  æœ€è¿‘50epæˆåŠŸç‡: {recent_success:.1%}")
            pbar.write(f"  æœ€è¿‘50epå¹³å‡å¥–åŠ±: {avg_reward:.1f}")
            pbar.write(f"  Memoryå¤§å°: {agent.memory.size}")
            
            if total_verify > 0:
                pbar.write(f"  POLARå®‰å…¨ç‡: {safe_count/total_verify:.1%}")
            
            # èƒ½åŠ›ä¼°è®¡ç»Ÿè®¡
            cap_stats = agent.get_capability_stats()
            pbar.write(f"  è·Ÿè¸ªç›®æ ‡æ•°: {cap_stats['num_goals_tracked']}")
            
            pbar.write("\n  è·ç¦»åŒºé—´æˆåŠŸç‡:")
            for dist, stats in sorted(cap_stats['distance_bin_stats'].items()):
                pbar.write(f"    <{dist:.1f}m: {stats['success_rate']:.1%} "
                          f"({stats['samples']}æ ·æœ¬)")
            
            pbar.write("="*60 + "\n")
            
            # ä¿å­˜æ¨¡å‹
            agent.save_gcpo(f"{model_dir}/episode_{episode_num}")
            pbar.write(f"âœ“ æ¨¡å‹å·²ä¿å­˜: episode_{episode_num}\n")
        
        # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤º
        pbar.set_postfix({
            'Ep': episode_num,
            'Success': f'{np.mean(success_history[-20:]):.1%}' if len(success_history) >= 20 else 'N/A'
        })
        
        env.close()
    
    pbar.close()
    
    # è®­ç»ƒå®Œæˆ
    print("\n" + "="*80)
    print("è®­ç»ƒå®Œæˆ!")
    print("="*80)
    print(f"æ€»Episodes: {episode_num}")
    print(f"æ€»Steps: {total_timesteps}")
    print(f"æ€»æˆåŠŸç‡: {np.mean(success_history):.1%}")
    print(f"æœ€è¿‘50epæˆåŠŸç‡: {np.mean(success_history[-50:]):.1%}")
    
    if total_verify > 0:
        print(f"\nPOLARéªŒè¯:")
        print(f"  æ€»éªŒè¯: {total_verify}")
        print(f"  å®‰å…¨ç‡: {safe_count/total_verify:.1%}")
    
    # èƒ½åŠ›ä¼°è®¡æœ€ç»ˆç»Ÿè®¡
    print(f"\nèƒ½åŠ›ä¼°è®¡ç»Ÿè®¡:")
    cap_stats = agent.get_capability_stats()
    print(f"  è·Ÿè¸ªç›®æ ‡æ•°: {cap_stats['num_goals_tracked']}")
    print(f"  è·ç¦»åŒºé—´æˆåŠŸç‡:")
    for dist, stats in sorted(cap_stats['distance_bin_stats'].items()):
        print(f"    <{dist:.1f}m: {stats['success_rate']:.1%} ({stats['samples']}æ ·æœ¬)")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_path = os.path.join(model_dir, "final")
    agent.save_gcpo(final_path)
    agent.save_weights(f"{final_path}/weights_for_polar.npz")
    print(f"\nâœ“ æœ€ç»ˆæ¨¡å‹: {final_path}")
    
    # ä¿å­˜è®­ç»ƒæ›²çº¿
    np.savez(
        f"{log_dir}/training_history.npz",
        rewards=rewards_history,
        successes=success_history,
        goals=np.array(goal_history)
    )
    print(f"âœ“ è®­ç»ƒæ•°æ®: {log_dir}/training_history.npz")
    
    return agent


def evaluate_gcpo(agent, test_goals, num_episodes_per_goal=5):
    """
    è¯„ä¼°GCPOåœ¨ä¸åŒç›®æ ‡ä¸Šçš„è¡¨ç°
    
    Args:
        agent: GCPOæ™ºèƒ½ä½“
        test_goals: æµ‹è¯•ç›®æ ‡åˆ—è¡¨
        num_episodes_per_goal: æ¯ä¸ªç›®æ ‡æµ‹è¯•çš„episodesæ•°
    
    Returns:
        results: è¯„ä¼°ç»“æœå­—å…¸
    """
    print("\n" + "="*60)
    print("è¯„ä¼°GCPOç­–ç•¥")
    print("="*60)
    
    results = {}
    
    for goal in test_goals:
        print(f"\næµ‹è¯•ç›®æ ‡: ({goal[0]:.2f}, {goal[1]:.2f})")
        
        env = ClearpathNavEnv(goal_pos=goal)
        
        success_count = 0
        rewards = []
        steps_list = []
        
        for ep in range(num_episodes_per_goal):
            state, _ = env.reset()
            episode_reward = 0
            episode_steps = 0
            
            done = False
            while not done and episode_steps < 256:
                action = agent.select_action(state)
                next_state, reward, done, truncated, info = env.step(action)
                
                episode_reward += reward
                episode_steps += 1
                state = next_state
                
                if done or truncated:
                    break
            
            success = info.get('goal_reached', False)
            if success:
                success_count += 1
            
            rewards.append(episode_reward)
            steps_list.append(episode_steps)
            
            print(f"  Ep {ep+1}: R={episode_reward:6.1f}, "
                  f"Steps={episode_steps:3d}, "
                  f"{'âœ…' if success else 'âŒ'}")
        
        env.close()
        
        # ç»Ÿè®¡
        results[tuple(goal)] = {
            'success_rate': success_count / num_episodes_per_goal,
            'avg_reward': np.mean(rewards),
            'avg_steps': np.mean(steps_list)
        }
        
        print(f"  æˆåŠŸç‡: {results[tuple(goal)]['success_rate']:.1%}")
    
    print("\n" + "="*60)
    print("è¯„ä¼°æ±‡æ€»:")
    print("="*60)
    for goal, stats in results.items():
        print(f"  {goal}: æˆåŠŸç‡={stats['success_rate']:.1%}, "
              f"å¹³å‡å¥–åŠ±={stats['avg_reward']:.1f}")
    print("="*60 + "\n")
    
    return results


if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='GCPOè®­ç»ƒ')
    
    # æ¨¡å¼é€‰æ‹©
    parser.add_argument('--mode', type=str, default='rl',
                       choices=['rl', 'evaluate'],
                       help='è®­ç»ƒæ¨¡å¼: rl=RLè®­ç»ƒ, evaluate=è¯„ä¼°')
    
    # RLè®­ç»ƒå‚æ•°
    parser.add_argument('--bc_model', type=str, default='./models/bc_pretrained',
                       help='BCé¢„è®­ç»ƒæ¨¡å‹è·¯å¾„')
    parser.add_argument('--timesteps', type=int, default=100000,
                       help='RLè®­ç»ƒæ€»æ­¥æ•°')
    parser.add_argument('--start_timesteps', type=int, default=5000,
                       help='åˆå§‹æ¢ç´¢æ­¥æ•°')
    parser.add_argument('--save_dir', type=str, default='./models/gcpo',
                       help='æ¨¡å‹ä¿å­˜ç›®å½•')
    
    # è¯„ä¼°å‚æ•°
    parser.add_argument('--eval_model', type=str, default=None,
                       help='è¯„ä¼°æ¨¡å‹è·¯å¾„')
    parser.add_argument('--eval_goals', type=str, default='1,1;2,2;5,-2',
                       help='è¯„ä¼°ç›®æ ‡,æ ¼å¼: x1,y1;x2,y2')
    
    args = parser.parse_args()
    
    try:
        config = GCPOConfig()
        
        if args.mode == 'rl':
            # RLè®­ç»ƒæ¨¡å¼
            print("="*70)
            print("GCPO RL Fine-tuning è®­ç»ƒ")
            print("="*70)
            
            # åˆ›å»ºæ™ºèƒ½ä½“
            agent = GCPOAgent(
                state_dim=12,
                action_dim=2,
                max_action=0.5,
                config=config
            )
            
            # åŠ è½½BCé¢„è®­ç»ƒæ¨¡å‹
            if os.path.exists(args.bc_model):
                print(f"\nåŠ è½½BCé¢„è®­ç»ƒæ¨¡å‹: {args.bc_model}")
                agent.load(args.bc_model)
                start_from_bc = True
            else:
                print(f"\nâš ï¸  BCæ¨¡å‹ä¸å­˜åœ¨: {args.bc_model}")
                print("å°†ä»éšæœºåˆå§‹åŒ–å¼€å§‹è®­ç»ƒ")
                start_from_bc = False
            
            # è®¾ç½®è®­ç»ƒå‚æ•°
            config.rl_total_timesteps = args.timesteps
            config.rl_start_timesteps = args.start_timesteps
            
            # å¼€å§‹RLè®­ç»ƒ
            agent = train_gcpo_rl_phase(
                agent, 
                config,
                start_from_bc=start_from_bc,
                save_dir=args.save_dir
            )
            
        elif args.mode == 'evaluate':
            # è¯„ä¼°æ¨¡å¼
            if args.eval_model is None:
                print("é”™è¯¯: è¯„ä¼°æ¨¡å¼éœ€è¦æŒ‡å®š --eval_model")
                sys.exit(1)
            
            print("="*70)
            print("GCPO ç­–ç•¥è¯„ä¼°")
            print("="*70)
            
            # åŠ è½½æ¨¡å‹
            agent = GCPOAgent(12, 2, 0.5, config)
            agent.load_gcpo(args.eval_model)
            
            # è§£ææµ‹è¯•ç›®æ ‡
            test_goals = []
            for goal_str in args.eval_goals.split(';'):
                x, y = map(float, goal_str.split(','))
                test_goals.append((x, y))
            
            # è¯„ä¼°
            results = evaluate_gcpo(agent, test_goals, num_episodes_per_goal=5)
        
        print("\nâœ… å…¨éƒ¨å®Œæˆ!")
        
    except KeyboardInterrupt:
        print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()