#!/usr/bin/env python3
"""
GCPO Goal5 ä¸“ç”¨è®­ç»ƒæµç¨‹
ä»BCé¢„è®­ç»ƒ(Goal5) â†’ RL Fine-tuning (ä¸“æ³¨Goal5åŠå…¶é‚»åŸŸ)
"""

import sys
import os
import numpy as np
from datetime import datetime
from tqdm import tqdm

sys.path.append(os.path.dirname(__file__))

from algorithms.gcpo_agent import GCPOAgent
from envs.clearpath_nav_env import ClearpathNavEnv
from utils.config import TD3Config
from adaptive_max_steps import AdaptiveStepsManager


class GCPOGoal5Config(TD3Config):
    """GCPO Goal5è®­ç»ƒé…ç½®"""
    
    # BCé¢„è®­ç»ƒå‚æ•°
    bc_epochs = 100
    bc_batch_size = 256
    bc_learning_rate = 3e-4
    bc_val_split = 0.1
    
    # RL Fine-tuningå‚æ•°
    rl_total_timesteps = 100000
    rl_start_timesteps = 3000  # Goal5å·²æœ‰BCåŸºç¡€,å‡å°‘éšæœºæ¢ç´¢
    
    # Goal5ç›®æ ‡åŒºåŸŸ
    goal5_center = (4.5, -2.0)  # Goal5ä¸­å¿ƒä½ç½®
    goal5_radius_small = 0.3    # å°æ‰°åŠ¨åŠå¾„
    goal5_radius_medium = 0.6   # ä¸­ç­‰æ‰°åŠ¨åŠå¾„
    goal5_radius_large = 1.0    # å¤§æ‰°åŠ¨åŠå¾„
    
    # è¯¾ç¨‹å­¦ä¹ å‚æ•°
    goal_space_bounds = [(0.5, 5.5), (-3.0, 3.5)]
    capability_window = 100
    target_capability = 0.5
    
    # èƒ½åŠ›è¯„ä¼°
    eval_interval = 5000
    eval_episodes = 10


def train_gcpo_goal5_rl_phase(
    agent, 
    config, 
    start_from_bc=True,
    save_dir='./models/gcpo_goal5'
):
    """
    GCPO Goal5 RL Fine-tuningé˜¶æ®µ
    ä¸“æ³¨äºGoal5åŠå…¶å‘¨å›´åŒºåŸŸçš„è®­ç»ƒ
    
    Args:
        agent: é¢„è®­ç»ƒçš„GCPOæ™ºèƒ½ä½“(Goal5 BC)
        config: é…ç½®
        start_from_bc: æ˜¯å¦ä»BCé¢„è®­ç»ƒå¼€å§‹
        save_dir: ä¿å­˜ç›®å½•
    
    Returns:
        agent: è®­ç»ƒåçš„æ™ºèƒ½ä½“
    """
    print("\n" + "="*80)
    print("ğŸš€ GCPO Goal5 RL Fine-tuning")
    print("="*80)
    
    if start_from_bc:
        print("  èµ·ç‚¹: Goal5 BCé¢„è®­ç»ƒç­–ç•¥")
    else:
        print("  èµ·ç‚¹: éšæœºç­–ç•¥")
    
    print(f"  ç›®æ ‡ä¸­å¿ƒ: ({config.goal5_center[0]:.1f}, {config.goal5_center[1]:.1f})")
    print(f"  æ€»æ­¥æ•°: {config.rl_total_timesteps}")
    print(f"  åˆå§‹æ¢ç´¢: {config.rl_start_timesteps}æ­¥")
    print(f"  è‡ªé€‚åº”è¯¾ç¨‹: âœ“ å¯ç”¨ (ä¸“æ³¨Goal5åŒºåŸŸ)")
    print(f"  è‡ªé€‚åº”æ­¥æ•°: âœ“ å¯ç”¨")
    print("="*80 + "\n")
    
    # åˆ‡æ¢åˆ°RLæ¨¡å¼
    agent.switch_to_rl_mode()
    
    # åˆ›å»ºæ—¥å¿—
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = os.path.join('./logs', f'gcpo_goal5_rl_{timestamp}')
    model_dir = os.path.join(save_dir, f'gcpo_goal5_rl_{timestamp}')
    os.makedirs(log_dir, exist_ok=True)
    os.makedirs(model_dir, exist_ok=True)
    
    # è®­ç»ƒç»Ÿè®¡
    total_timesteps = 0
    episode_num = 0
    rewards_history = []
    success_history = []
    goal_history = []
    stage_history = []  # è®°å½•æ¯ä¸ªepisodeçš„è¯¾ç¨‹é˜¶æ®µ
    
    # POLARéªŒè¯ç»Ÿè®¡
    safe_count = 0
    total_verify = 0
    
    # ğŸ”¥ åˆ›å»ºè‡ªé€‚åº”æ­¥æ•°ç®¡ç†å™¨
    # ä¼˜å…ˆä½¿ç”¨Goal5æ•°æ®,å¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨å…¨å±€æ•°æ®
    if os.path.exists('./demonstrations_goal5'):
        steps_manager = AdaptiveStepsManager(demo_dir='./demonstrations_goal5')
        print("âœ“ ä½¿ç”¨Goal5ä¸“ç”¨æ¼”ç¤ºæ•°æ®åˆå§‹åŒ–è‡ªé€‚åº”æ­¥æ•°ç®¡ç†å™¨")
    else:
        steps_manager = AdaptiveStepsManager(demo_dir='./demonstrations')
        print("âœ“ ä½¿ç”¨å…¨å±€æ¼”ç¤ºæ•°æ®åˆå§‹åŒ–è‡ªé€‚åº”æ­¥æ•°ç®¡ç†å™¨")
    
    # ğŸ”¥ Goal5æ¸è¿›å¼è¯¾ç¨‹è®¾è®¡
    goal5_center = np.array(config.goal5_center, dtype=np.float32)
    
    print(f"\nğŸ“ Goal5æ¸è¿›å¼è¯¾ç¨‹:")
    print(f"  é˜¶æ®µ1 (0-25kæ­¥):   å›ºå®šGoal5 - å·©å›ºBCå­¦ä¹ ")
    print(f"  é˜¶æ®µ2 (25k-50kæ­¥): å°æ‰°åŠ¨ - Goal5 Â± {config.goal5_radius_small}m")
    print(f"  é˜¶æ®µ3 (50k-75kæ­¥): ä¸­æ‰°åŠ¨ - Goal5 Â± {config.goal5_radius_medium}m")
    print(f"  é˜¶æ®µ4 (75k+æ­¥):    å¤§æ‰°åŠ¨ - Goal5 Â± {config.goal5_radius_large}m")
    print()
    
    # åˆ›å»ºè¿›åº¦æ¡
    pbar = tqdm(total=config.rl_total_timesteps, desc="Goal5 RLè®­ç»ƒ")
    
    # ä¸»è®­ç»ƒå¾ªç¯
    while total_timesteps < config.rl_total_timesteps:
        # ğŸ”¥ Goal5æ¸è¿›å¼è¯¾ç¨‹: æ ¹æ®è®­ç»ƒè¿›åº¦é€‰æ‹©ç›®æ ‡
        if total_timesteps < 25000:
            # é˜¶æ®µ1: å›ºå®šGoal5 (å·©å›ºBCå­¦ä¹ çš„çŸ¥è¯†)
            curriculum_goal = goal5_center.copy()
            stage = "å›ºå®š"
            
        elif total_timesteps < 50000:
            # é˜¶æ®µ2: Goal5 + å°æ‰°åŠ¨ (å¼€å§‹å±€éƒ¨æ³›åŒ–)
            noise = np.random.uniform(-config.goal5_radius_small, 
                                     config.goal5_radius_small, size=2)
            curriculum_goal = goal5_center + noise
            stage = "å°æ‰°åŠ¨"
            
        elif total_timesteps < 75000:
            # é˜¶æ®µ3: Goal5 + ä¸­ç­‰æ‰°åŠ¨ (è¿›ä¸€æ­¥æ³›åŒ–)
            noise = np.random.uniform(-config.goal5_radius_medium, 
                                     config.goal5_radius_medium, size=2)
            curriculum_goal = goal5_center + noise
            stage = "ä¸­æ‰°åŠ¨"
            
        else:
            # é˜¶æ®µ4: Goal5 + å¤§æ‰°åŠ¨ (å……åˆ†æ¢ç´¢Goal5é‚»åŸŸ)
            noise = np.random.uniform(-config.goal5_radius_large, 
                                     config.goal5_radius_large, size=2)
            curriculum_goal = goal5_center + noise
            stage = "å¤§æ‰°åŠ¨"
        
        # ç¡®ä¿ç›®æ ‡åœ¨åˆç†èŒƒå›´å†…
        curriculum_goal = np.clip(
            curriculum_goal,
            [config.goal_space_bounds[0][0], config.goal_space_bounds[1][0]],
            [config.goal_space_bounds[0][1], config.goal_space_bounds[1][1]]
        )
        
        # ğŸ”¥ ä½¿ç”¨è‡ªé€‚åº”æ­¥æ•°ç®¡ç†å™¨
        adaptive_max_steps = steps_manager.get_max_steps(curriculum_goal)
        
        # åˆ›å»ºç¯å¢ƒ
        env = ClearpathNavEnv(
            goal_pos=tuple(curriculum_goal), 
            max_steps=adaptive_max_steps
        )
        
        # é‡ç½®ç¯å¢ƒ
        state, _ = env.reset()
        episode_reward = 0
        episode_steps = 0
        
        # Episodeå¾ªç¯
        for step in range(adaptive_max_steps):
            # é€‰æ‹©åŠ¨ä½œ
            if total_timesteps < config.rl_start_timesteps:
                # åˆå§‹æ¢ç´¢é˜¶æ®µ:éšæœºåŠ¨ä½œ
                action = env.action_space.sample()
            else:
                # ä½¿ç”¨ç­–ç•¥
                action = agent.select_action(state)
                # æ·»åŠ æ¢ç´¢å™ªå£°
                noise = np.random.normal(0, config.max_action * config.expl_noise, 
                                        size=config.action_dim)
                action = (action + noise).clip(-config.max_action, config.max_action)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, done, truncated, info = env.step(action)
            done_bool = float(done) if episode_steps < adaptive_max_steps else 0
            
            # å­˜å‚¨ç»éªŒ
            agent.store_transition(state, action, next_state, reward, done_bool)
            
            state = next_state
            episode_reward += reward
            episode_steps += 1
            total_timesteps += 1
            
            # è®­ç»ƒç½‘ç»œ
            if total_timesteps >= config.rl_start_timesteps:
                agent.train()
            
            # POLARéªŒè¯
            if total_timesteps > 0 and total_timesteps % config.verify_interval == 0:
                try:
                    is_safe, _ = agent.verify_safety(
                        state,
                        observation_error=config.observation_error,
                        bern_order=config.bern_order,
                        error_steps=config.error_steps
                    )
                    total_verify += 1
                    if is_safe:
                        safe_count += 1
                except:
                    pass
            
            pbar.update(1)
            
            if done or truncated:
                break
        
        # Episodeç»“æŸå¤„ç†
        episode_success = info.get('goal_reached', False)
        
        # æ›´æ–°èƒ½åŠ›ä¼°è®¡
        agent.update_capability(curriculum_goal, episode_success)
        
        # ğŸ”¥ æ›´æ–°è‡ªé€‚åº”æ­¥æ•°ç»Ÿè®¡
        steps_manager.update_online(curriculum_goal, episode_steps, episode_success)
        
        # è®°å½•ç»Ÿè®¡
        rewards_history.append(episode_reward)
        success_history.append(episode_success)
        goal_history.append(curriculum_goal.copy())
        stage_history.append(stage)
        
        # è®¡ç®—åˆ°Goal5ä¸­å¿ƒçš„è·ç¦»
        distance_to_goal5 = np.linalg.norm(curriculum_goal - goal5_center)
        
        # æ‰“å°ä¿¡æ¯
        capability = agent.capability_estimator.estimate_capability(curriculum_goal)
        msg = (f"Ep {episode_num} [{stage:4s}]: "
               f"Goal=({curriculum_goal[0]:.2f},{curriculum_goal[1]:.2f}), "
               f"Dist2G5={distance_to_goal5:.2f}m, "
               f"R={episode_reward:.1f}, "
               f"Steps={episode_steps}/{adaptive_max_steps}, "
               f"Cap={capability:.1%}")
        
        if episode_success:
            msg += " ğŸ¯"
        elif info.get('collision'):
            msg += " âš ï¸"
        elif episode_steps >= adaptive_max_steps:
            msg += " â±ï¸"
        
        pbar.write(msg)
        
        episode_num += 1
        
        # å®šæœŸç»Ÿè®¡å’Œä¿å­˜
        if episode_num % 50 == 0:
            recent_success = np.mean(success_history[-50:])
            avg_reward = np.mean(rewards_history[-50:])
            
            # ç»Ÿè®¡å„é˜¶æ®µçš„æˆåŠŸç‡
            recent_stages = stage_history[-50:]
            recent_successes = success_history[-50:]
            stage_stats = {}
            for s, succ in zip(recent_stages, recent_successes):
                if s not in stage_stats:
                    stage_stats[s] = {'success': 0, 'total': 0}
                stage_stats[s]['total'] += 1
                if succ:
                    stage_stats[s]['success'] += 1
            
            pbar.write("\n" + "="*70)
            pbar.write(f"Episode {episode_num} ç»Ÿè®¡ (æ€»æ­¥æ•°: {total_timesteps}):")
            pbar.write(f"  æœ€è¿‘50epæˆåŠŸç‡: {recent_success:.1%}")
            pbar.write(f"  æœ€è¿‘50epå¹³å‡å¥–åŠ±: {avg_reward:.1f}")
            pbar.write(f"  Memoryå¤§å°: {agent.memory.size}")
            
            if total_verify > 0:
                pbar.write(f"  POLARå®‰å…¨ç‡: {safe_count/total_verify:.1%}")
            
            # å„é˜¶æ®µæˆåŠŸç‡
            pbar.write(f"\n  å„é˜¶æ®µæˆåŠŸç‡ (æœ€è¿‘50ep):")
            for stage_name, stats in stage_stats.items():
                sr = stats['success'] / stats['total'] if stats['total'] > 0 else 0
                pbar.write(f"    {stage_name:6s}: {sr:.1%} ({stats['success']}/{stats['total']})")
            
            # èƒ½åŠ›ä¼°è®¡ç»Ÿè®¡
            cap_stats = agent.get_capability_stats()
            pbar.write(f"\n  èƒ½åŠ›ä¼°è®¡:")
            pbar.write(f"    è·Ÿè¸ªç›®æ ‡æ•°: {cap_stats['num_goals_tracked']}")
            
            # ğŸ”¥ è‡ªé€‚åº”æ­¥æ•°ç»Ÿè®¡
            steps_stats = steps_manager.get_statistics_summary()
            pbar.write(f"\n  è‡ªé€‚åº”æ­¥æ•°ç»Ÿè®¡:")
            pbar.write(f"    è·Ÿè¸ªç›®æ ‡æ•°: {steps_stats['num_goals']}")
            pbar.write(f"    å¹³å‡æ­¥æ•°: {steps_stats['avg_steps_overall']:.0f}")
            pbar.write(f"    æ­¥æ•°èŒƒå›´: {steps_stats['min_steps_overall']:.0f}-{steps_stats['max_steps_overall']:.0f}")
            
            # Goal5ä¸­å¿ƒåŒºåŸŸçš„èƒ½åŠ›
            goal5_capability = agent.capability_estimator.estimate_capability(goal5_center)
            pbar.write(f"\n  Goal5ä¸­å¿ƒèƒ½åŠ›: {goal5_capability:.1%}")
            
            pbar.write("\n  è·ç¦»åŒºé—´æˆåŠŸç‡:")
            for dist, stats in sorted(cap_stats['distance_bin_stats'].items()):
                pbar.write(f"    <{dist:.1f}m: {stats['success_rate']:.1%} "
                          f"({stats['samples']}æ ·æœ¬)")
            
            pbar.write("="*70 + "\n")
            
            # ä¿å­˜æ¨¡å‹
            agent.save_gcpo(f"{model_dir}/episode_{episode_num}")
            pbar.write(f"âœ“ æ¨¡å‹å·²ä¿å­˜: episode_{episode_num}\n")
        
        # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤º
        recent_20_success = np.mean(success_history[-20:]) if len(success_history) >= 20 else 0
        pbar.set_postfix({
            'Ep': episode_num,
            'Stage': stage,
            'Success': f'{recent_20_success:.1%}'
        })
        
        env.close()
    
    pbar.close()
    
    # è®­ç»ƒå®Œæˆ
    print("\n" + "="*80)
    print("Goal5 è®­ç»ƒå®Œæˆ!")
    print("="*80)
    print(f"æ€»Episodes: {episode_num}")
    print(f"æ€»Steps: {total_timesteps}")
    print(f"æ€»æˆåŠŸç‡: {np.mean(success_history):.1%}")
    print(f"æœ€è¿‘50epæˆåŠŸç‡: {np.mean(success_history[-50:]):.1%}")
    
    # å„é˜¶æ®µç»Ÿè®¡
    print(f"\nå„é˜¶æ®µæ•´ä½“ç»Ÿè®¡:")
    unique_stages = list(dict.fromkeys(stage_history))  # ä¿æŒé¡ºåºçš„å»é‡
    for stage_name in unique_stages:
        stage_indices = [i for i, s in enumerate(stage_history) if s == stage_name]
        stage_successes = [success_history[i] for i in stage_indices]
        if len(stage_successes) > 0:
            print(f"  {stage_name:6s}: {np.mean(stage_successes):.1%} "
                  f"({sum(stage_successes)}/{len(stage_successes)})")
    
    if total_verify > 0:
        print(f"\nPOLARéªŒè¯:")
        print(f"  æ€»éªŒè¯: {total_verify}")
        print(f"  å®‰å…¨ç‡: {safe_count/total_verify:.1%}")
    
    # Goal5ä¸­å¿ƒåŒºåŸŸçš„æœ€ç»ˆèƒ½åŠ›
    goal5_capability = agent.capability_estimator.estimate_capability(goal5_center)
    print(f"\nGoal5ä¸­å¿ƒæœ€ç»ˆèƒ½åŠ›: {goal5_capability:.1%}")
    
    # èƒ½åŠ›ä¼°è®¡æœ€ç»ˆç»Ÿè®¡
    print(f"\nèƒ½åŠ›ä¼°è®¡ç»Ÿè®¡:")
    cap_stats = agent.get_capability_stats()
    print(f"  è·Ÿè¸ªç›®æ ‡æ•°: {cap_stats['num_goals_tracked']}")
    print(f"  è·ç¦»åŒºé—´æˆåŠŸç‡:")
    for dist, stats in sorted(cap_stats['distance_bin_stats'].items()):
        print(f"    <{dist:.1f}m: {stats['success_rate']:.1%} ({stats['samples']}æ ·æœ¬)")
    
    # ğŸ”¥ è‡ªé€‚åº”æ­¥æ•°æœ€ç»ˆç»Ÿè®¡
    print(f"\nè‡ªé€‚åº”æ­¥æ•°æœ€ç»ˆç»Ÿè®¡:")
    steps_stats = steps_manager.get_statistics_summary()
    print(f"  è·Ÿè¸ªç›®æ ‡æ•°: {steps_stats['num_goals']}")
    print(f"  å¹³å‡æ­¥æ•°: {steps_stats['avg_steps_overall']:.0f}")
    print(f"  æ­¥æ•°èŒƒå›´: {steps_stats['min_steps_overall']:.0f}-{steps_stats['max_steps_overall']:.0f}")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_path = os.path.join(model_dir, "final")
    agent.save_gcpo(final_path)
    agent.save_weights(f"{final_path}/weights_for_polar.npz")
    print(f"\nâœ“ æœ€ç»ˆæ¨¡å‹: {final_path}")
    
    # ğŸ”¥ ä¿å­˜è‡ªé€‚åº”æ­¥æ•°ç»Ÿè®¡
    steps_manager.save(f"{model_dir}/adaptive_steps_goal5.pkl")
    
    # ä¿å­˜è®­ç»ƒæ›²çº¿
    np.savez(
        f"{log_dir}/training_history.npz",
        rewards=rewards_history,
        successes=success_history,
        goals=np.array(goal_history),
        stages=stage_history
    )
    print(f"âœ“ è®­ç»ƒæ•°æ®: {log_dir}/training_history.npz")
    
    # ä¿å­˜Goal5ä¸“ç”¨ä¿¡æ¯
    with open(f"{model_dir}/goal5_info.txt", 'w') as f:
        f.write(f"Goal5 Training Information\n")
        f.write(f"{'='*50}\n")
        f.write(f"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"Goal5 Center: {config.goal5_center}\n")
        f.write(f"Total Episodes: {episode_num}\n")
        f.write(f"Total Timesteps: {total_timesteps}\n")
        f.write(f"Overall Success Rate: {np.mean(success_history):.1%}\n")
        f.write(f"Recent 50ep Success Rate: {np.mean(success_history[-50:]):.1%}\n")
        f.write(f"Goal5 Center Capability: {goal5_capability:.1%}\n")
        f.write(f"\nStage Statistics:\n")
        for stage_name in unique_stages:
            stage_indices = [i for i, s in enumerate(stage_history) if s == stage_name]
            stage_successes = [success_history[i] for i in stage_indices]
            if len(stage_successes) > 0:
                f.write(f"  {stage_name}: {np.mean(stage_successes):.1%} "
                       f"({sum(stage_successes)}/{len(stage_successes)})\n")
    
    print(f"âœ“ Goal5ä¿¡æ¯: {model_dir}/goal5_info.txt")
    
    return agent


def evaluate_gcpo_goal5(agent, config, num_episodes=20):
    """
    è¯„ä¼°GCPOåœ¨Goal5åŠå…¶é‚»åŸŸçš„è¡¨ç°
    
    Args:
        agent: GCPOæ™ºèƒ½ä½“
        config: é…ç½®å¯¹è±¡
        num_episodes: æ¯ä¸ªæµ‹è¯•ç‚¹çš„episodesæ•°
    
    Returns:
        results: è¯„ä¼°ç»“æœå­—å…¸
    """
    print("\n" + "="*70)
    print("è¯„ä¼°GCPO Goal5ç­–ç•¥")
    print("="*70)
    
    # ğŸ”¥ è¯„ä¼°æ—¶ä¹Ÿä½¿ç”¨è‡ªé€‚åº”æ­¥æ•°
    if os.path.exists('./demonstrations_goal5'):
        steps_manager = AdaptiveStepsManager(demo_dir='./demonstrations_goal5')
    else:
        steps_manager = AdaptiveStepsManager(demo_dir='./demonstrations')
    
    goal5_center = np.array(config.goal5_center, dtype=np.float32)
    
    # æµ‹è¯•ç‚¹: Goal5ä¸­å¿ƒ + å‘¨å›´ä¸åŒè·ç¦»çš„ç‚¹
    test_configs = [
        ("Goal5ä¸­å¿ƒ", goal5_center),
        ("å°æ‰°åŠ¨+X", goal5_center + [0.3, 0.0]),
        ("å°æ‰°åŠ¨-X", goal5_center + [-0.3, 0.0]),
        ("å°æ‰°åŠ¨+Y", goal5_center + [0.0, 0.3]),
        ("å°æ‰°åŠ¨-Y", goal5_center + [0.0, -0.3]),
        ("ä¸­æ‰°åŠ¨", goal5_center + [0.5, 0.5]),
        ("å¤§æ‰°åŠ¨", goal5_center + [0.8, -0.8]),
    ]
    
    results = {}
    
    for name, goal in test_configs:
        print(f"\næµ‹è¯•ç‚¹: {name} - ({goal[0]:.2f}, {goal[1]:.2f})")
        
        # è·å–è‡ªé€‚åº”æ­¥æ•°
        adaptive_max_steps = steps_manager.get_max_steps(goal)
        print(f"  è‡ªé€‚åº”max_steps: {adaptive_max_steps}")
        
        env = ClearpathNavEnv(goal_pos=tuple(goal), max_steps=adaptive_max_steps)
        
        success_count = 0
        collision_count = 0
        timeout_count = 0
        rewards = []
        steps_list = []
        distances = []
        
        for ep in range(num_episodes):
            state, _ = env.reset()
            episode_reward = 0
            episode_steps = 0
            
            done = False
            while not done and episode_steps < adaptive_max_steps:
                action = agent.select_action(state)
                next_state, reward, done, truncated, info = env.step(action)
                
                episode_reward += reward
                episode_steps += 1
                state = next_state
                
                if done or truncated:
                    break
            
            success = info.get('goal_reached', False)
            collision = info.get('collision', False)
            
            if success:
                success_count += 1
            if collision:
                collision_count += 1
            if episode_steps >= adaptive_max_steps:
                timeout_count += 1
            
            rewards.append(episode_reward)
            steps_list.append(episode_steps)
            distances.append(info.get('distance', 0))
            
            status = 'âœ…' if success else ('ğŸ’¥' if collision else 'â±ï¸')
            print(f"  Ep {ep+1:2d}: R={episode_reward:7.1f}, "
                  f"Steps={episode_steps:3d}/{adaptive_max_steps}, "
                  f"Dist={distances[-1]:5.2f}m {status}")
        
        env.close()
        
        # ç»Ÿè®¡
        results[name] = {
            'goal_position': tuple(goal),
            'success_rate': success_count / num_episodes,
            'collision_rate': collision_count / num_episodes,
            'timeout_rate': timeout_count / num_episodes,
            'avg_reward': np.mean(rewards),
            'avg_steps': np.mean(steps_list),
            'avg_final_distance': np.mean(distances),
            'min_distance': np.min(distances)
        }
        
        print(f"  ç»Ÿè®¡: æˆåŠŸ={results[name]['success_rate']:.1%}, "
              f"ç¢°æ’={results[name]['collision_rate']:.1%}, "
              f"è¶…æ—¶={results[name]['timeout_rate']:.1%}")
    
    print("\n" + "="*70)
    print("Goal5 è¯„ä¼°æ±‡æ€»:")
    print("="*70)
    for name, stats in results.items():
        print(f"\n  {name}:")
        print(f"    ä½ç½®: {stats['goal_position']}")
        print(f"    æˆåŠŸç‡: {stats['success_rate']:.1%}")
        print(f"    å¹³å‡å¥–åŠ±: {stats['avg_reward']:.1f}")
        print(f"    å¹³å‡æ­¥æ•°: {stats['avg_steps']:.1f}")
        print(f"    æœ€ç»ˆè·ç¦»: {stats['avg_final_distance']:.2f}m (æœ€å°={stats['min_distance']:.2f}m)")
    print("="*70 + "\n")
    
    return results


if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='GCPO Goal5ä¸“ç”¨è®­ç»ƒ')
    
    # æ¨¡å¼é€‰æ‹©
    parser.add_argument('--mode', type=str, default='rl',
                       choices=['rl', 'evaluate'],
                       help='è®­ç»ƒæ¨¡å¼: rl=RLè®­ç»ƒ, evaluate=è¯„ä¼°')
    
    # RLè®­ç»ƒå‚æ•°
    parser.add_argument('--bc_model', type=str, default='./models/bc_pretrained_goal5',
                       help='Goal5 BCé¢„è®­ç»ƒæ¨¡å‹è·¯å¾„')
    parser.add_argument('--timesteps', type=int, default=100000,
                       help='RLè®­ç»ƒæ€»æ­¥æ•°')
    parser.add_argument('--start_timesteps', type=int, default=3000,
                       help='åˆå§‹æ¢ç´¢æ­¥æ•°')
    parser.add_argument('--save_dir', type=str, default='./models/gcpo_goal5',
                       help='æ¨¡å‹ä¿å­˜ç›®å½•')
    
    # Goal5å‚æ•°
    parser.add_argument('--goal5_x', type=float, default=4.5,
                       help='Goal5 Xåæ ‡')
    parser.add_argument('--goal5_y', type=float, default=-2.0,
                       help='Goal5 Yåæ ‡')
    
    # è¯„ä¼°å‚æ•°
    parser.add_argument('--eval_model', type=str, default=None,
                       help='è¯„ä¼°æ¨¡å‹è·¯å¾„')
    parser.add_argument('--eval_episodes', type=int, default=20,
                       help='æ¯ä¸ªæµ‹è¯•ç‚¹çš„episodesæ•°')
    
    args = parser.parse_args()
    
    try:
        config = GCPOGoal5Config()
        config.goal5_center = (args.goal5_x, args.goal5_y)
        
        if args.mode == 'rl':
            # RLè®­ç»ƒæ¨¡å¼
            print("="*70)
            print("GCPO Goal5 RL Fine-tuning è®­ç»ƒ")
            print("="*70)
            print(f"Goal5ä½ç½®: ({args.goal5_x}, {args.goal5_y})")
            
            # åˆ›å»ºæ™ºèƒ½ä½“
            agent = GCPOAgent(
                state_dim=12,
                action_dim=2,
                max_action=0.5,
                config=config
            )
            
            # åŠ è½½Goal5 BCé¢„è®­ç»ƒæ¨¡å‹
            if os.path.exists(args.bc_model):
                print(f"\nåŠ è½½Goal5 BCé¢„è®­ç»ƒæ¨¡å‹: {args.bc_model}")
                agent.load(args.bc_model)
                start_from_bc = True
            else:
                print(f"\nâš ï¸  Goal5 BCæ¨¡å‹ä¸å­˜åœ¨: {args.bc_model}")
                print("å°†ä»éšæœºåˆå§‹åŒ–å¼€å§‹è®­ç»ƒ")
                start_from_bc = False
            
            # è®¾ç½®è®­ç»ƒå‚æ•°
            config.rl_total_timesteps = args.timesteps
            config.rl_start_timesteps = args.start_timesteps
            
            # å¼€å§‹RLè®­ç»ƒ
            agent = train_gcpo_goal5_rl_phase(
                agent, 
                config,
                start_from_bc=start_from_bc,
                save_dir=args.save_dir
            )
            
        elif args.mode == 'evaluate':
            # è¯„ä¼°æ¨¡å¼
            if args.eval_model is None:
                print("é”™è¯¯: è¯„ä¼°æ¨¡å¼éœ€è¦æŒ‡å®š --eval_model")
                sys.exit(1)
            
            print("="*70)
            print("GCPO Goal5 ç­–ç•¥è¯„ä¼°")
            print("="*70)
            
            # åŠ è½½æ¨¡å‹
            agent = GCPOAgent(12, 2, 0.5, config)
            agent.load_gcpo(args.eval_model)
            
            # è¯„ä¼°
            results = evaluate_gcpo_goal5(agent, config, num_episodes=args.eval_episodes)
            
            # ä¿å­˜è¯„ä¼°ç»“æœ
            eval_save_path = os.path.join(args.eval_model, 'evaluation_results_goal5.npz')
            
            # å‡†å¤‡ä¿å­˜çš„æ•°æ®
            save_data = {}
            for name, stats in results.items():
                for key, value in stats.items():
                    save_data[f"{name}_{key}"] = value
            
            np.savez(eval_save_path, **save_data)
            print(f"âœ“ è¯„ä¼°ç»“æœå·²ä¿å­˜: {eval_save_path}")
        
        print("\nâœ… å…¨éƒ¨å®Œæˆ!")
        
    except KeyboardInterrupt:
        print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()