#!/usr/bin/env python3
"""
GCPO Goal5 è®­ç»ƒ - å¸¦BCæ­£åˆ™åŒ–é˜²æ­¢é—å¿˜
å…³é”®æ”¹è¿›: åœ¨RLè®­ç»ƒæ—¶ä¿æŒBCç­–ç•¥çº¦æŸ
"""

import sys
import os
import numpy as np
import torch
from datetime import datetime
from tqdm import tqdm

sys.path.append(os.path.dirname(__file__))

from algorithms.gcpo_agent import GCPOAgent
from envs.clearpath_nav_env import ClearpathNavEnv
from utils.config import TD3Config
from adaptive_max_steps import AdaptiveStepsManager


class GCPOGoal5ConfigWithBCReg(TD3Config):
    """GCPO Goal5è®­ç»ƒé…ç½® - å¸¦BCæ­£åˆ™åŒ–"""
    
    # BCé¢„è®­ç»ƒå‚æ•°
    bc_epochs = 100
    bc_batch_size = 256
    bc_learning_rate = 3e-4
    bc_val_split = 0.1
    
    # ğŸ”¥ BCæ­£åˆ™åŒ–å‚æ•° (é˜²æ­¢é—å¿˜çš„å…³é”®!)
    bc_regularization = True
    bc_reg_lambda = 0.1  # BCæ­£åˆ™åŒ–æƒé‡ (å¯è°ƒ: 0.05-0.5)
    bc_demo_dir = './demonstrations_goal5'  # BCæ¼”ç¤ºæ•°æ®ç›®å½•
    bc_reg_batch_size = 64  # æ¯æ¬¡è®­ç»ƒæ—¶é‡‡æ ·çš„BCæ•°æ®é‡
    
    # RL Fine-tuningå‚æ•°
    rl_total_timesteps = 100000
    rl_start_timesteps = 1000  # ğŸ”¥ å‡å°‘åˆ°1000æ­¥ (æœ‰BCåŸºç¡€ä¸éœ€è¦å¤ªå¤šéšæœºæ¢ç´¢)
    
    # ğŸ”¥ é™ä½å­¦ä¹ ç‡ (é˜²æ­¢ç ´åBCçŸ¥è¯†)
    actor_lr = 1e-4  # ä»3e-4é™åˆ°1e-4
    critic_lr = 1e-3  # ä¿æŒä¸å˜
    
    # ğŸ”¥ é™ä½æ¢ç´¢å™ªå£° (BCå·²ç»æä¾›å¥½çš„åŸºç¡€ç­–ç•¥)
    expl_noise = 0.05  # ä»0.1é™åˆ°0.05
    
    # Goal5ç›®æ ‡åŒºåŸŸ
    goal5_center = (4.5, -2.0)
    goal5_radius_small = 0.3
    goal5_radius_medium = 0.6
    goal5_radius_large = 1.0
    
    # è¯¾ç¨‹å­¦ä¹ å‚æ•°
    goal_space_bounds = [(0.5, 5.5), (-3.0, 3.5)]
    capability_window = 100
    target_capability = 0.5
    
    # èƒ½åŠ›è¯„ä¼°
    eval_interval = 5000
    eval_episodes = 10


class BCRegularizer:
    """BCæ­£åˆ™åŒ–å™¨ - é˜²æ­¢ç¾éš¾æ€§é—å¿˜"""
    
    def __init__(self, demo_dir, goal_idx=5, device='cuda'):
        """
        åˆå§‹åŒ–BCæ­£åˆ™åŒ–å™¨
        
        Args:
            demo_dir: æ¼”ç¤ºæ•°æ®ç›®å½•
            goal_idx: ç›®æ ‡ç´¢å¼•
            device: è®¾å¤‡
        """
        self.device = device
        self.demo_states = []
        self.demo_actions = []
        
        # åŠ è½½æ¼”ç¤ºæ•°æ®
        import glob
        pattern = os.path.join(demo_dir, f'demo_goal{goal_idx}_num*.npz')
        demo_files = sorted(glob.glob(pattern))
        
        if len(demo_files) == 0:
            print(f"âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°BCæ¼”ç¤ºæ•°æ®ï¼ŒBCæ­£åˆ™åŒ–å°†è¢«ç¦ç”¨")
            return
        
        print(f"\nğŸ”¥ åŠ è½½BCæ­£åˆ™åŒ–æ•°æ®:")
        print(f"  ç›®å½•: {demo_dir}")
        print(f"  æ–‡ä»¶æ•°: {len(demo_files)}")
        
        for demo_file in demo_files:
            data = np.load(demo_file)
            self.demo_states.extend(data['states'])
            self.demo_actions.extend(data['actions'])
        
        self.demo_states = torch.FloatTensor(np.array(self.demo_states)).to(device)
        self.demo_actions = torch.FloatTensor(np.array(self.demo_actions)).to(device)
        
        print(f"  æ€»æ ·æœ¬æ•°: {len(self.demo_states)}")
        print(f"  BCæ­£åˆ™åŒ–å·²å¯ç”¨ âœ“\n")
    
    def compute_bc_loss(self, actor, batch_size=64):
        """
        è®¡ç®—BCæ­£åˆ™åŒ–æŸå¤±
        
        Args:
            actor: Actorç½‘ç»œ
            batch_size: æ‰¹æ¬¡å¤§å°
        
        Returns:
            bc_loss: BCæ­£åˆ™åŒ–æŸå¤±
        """
        if len(self.demo_states) == 0:
            return torch.tensor(0.0).to(self.device)
        
        # éšæœºé‡‡æ ·BCæ•°æ®
        indices = np.random.randint(0, len(self.demo_states), size=batch_size)
        states = self.demo_states[indices]
        actions = self.demo_actions[indices]
        
        # è®¡ç®—å½“å‰ç­–ç•¥çš„åŠ¨ä½œ
        predicted_actions = actor(states)
        
        # MSEæŸå¤±
        bc_loss = torch.nn.functional.mse_loss(predicted_actions, actions)
        
        return bc_loss


def train_gcpo_goal5_with_bc_reg(
    agent, 
    config, 
    bc_regularizer,
    start_from_bc=True,
    save_dir='./models/gcpo_goal5_bc_reg'
):
    """
    GCPO Goal5 RL Fine-tuning - å¸¦BCæ­£åˆ™åŒ–
    
    Args:
        agent: é¢„è®­ç»ƒçš„GCPOæ™ºèƒ½ä½“
        config: é…ç½®
        bc_regularizer: BCæ­£åˆ™åŒ–å™¨
        start_from_bc: æ˜¯å¦ä»BCé¢„è®­ç»ƒå¼€å§‹
        save_dir: ä¿å­˜ç›®å½•
    """
    print("\n" + "="*80)
    print("ğŸš€ GCPO Goal5 RL Fine-tuning (å¸¦BCæ­£åˆ™åŒ–)")
    print("="*80)
    
    if start_from_bc:
        print("  èµ·ç‚¹: Goal5 BCé¢„è®­ç»ƒç­–ç•¥")
    else:
        print("  èµ·ç‚¹: éšæœºç­–ç•¥")
    
    print(f"  ç›®æ ‡ä¸­å¿ƒ: ({config.goal5_center[0]:.1f}, {config.goal5_center[1]:.1f})")
    print(f"  æ€»æ­¥æ•°: {config.rl_total_timesteps}")
    print(f"  åˆå§‹æ¢ç´¢: {config.rl_start_timesteps}æ­¥")
    print(f"  ğŸ”¥ BCæ­£åˆ™åŒ–: âœ“ å¯ç”¨ (lambda={config.bc_reg_lambda})")
    print(f"  ğŸ”¥ Actorå­¦ä¹ ç‡: {config.actor_lr} (é™ä½ä»¥ä¿æŠ¤BCçŸ¥è¯†)")
    print(f"  ğŸ”¥ æ¢ç´¢å™ªå£°: {config.expl_noise} (é™ä½ä»¥åˆ©ç”¨BCç­–ç•¥)")
    print("="*80 + "\n")
    
    # ğŸ”¥ åº”ç”¨é™ä½çš„å­¦ä¹ ç‡
    for param_group in agent.actor_optimizer.param_groups:
        param_group['lr'] = config.actor_lr
    
    # åˆ‡æ¢åˆ°RLæ¨¡å¼
    agent.switch_to_rl_mode()
    
    # åˆ›å»ºæ—¥å¿—
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = os.path.join('./logs', f'gcpo_goal5_bc_reg_{timestamp}')
    model_dir = os.path.join(save_dir, f'gcpo_goal5_bc_reg_{timestamp}')
    os.makedirs(log_dir, exist_ok=True)
    os.makedirs(model_dir, exist_ok=True)
    
    # è®­ç»ƒç»Ÿè®¡
    total_timesteps = 0
    episode_num = 0
    rewards_history = []
    success_history = []
    goal_history = []
    stage_history = []
    
    # BCæ­£åˆ™åŒ–ç»Ÿè®¡
    bc_loss_history = []
    
    # POLARéªŒè¯ç»Ÿè®¡
    safe_count = 0
    total_verify = 0
    
    # è‡ªé€‚åº”æ­¥æ•°ç®¡ç†å™¨
    if os.path.exists('./demonstrations_goal5'):
        steps_manager = AdaptiveStepsManager(demo_dir='./demonstrations_goal5')
    else:
        steps_manager = AdaptiveStepsManager(demo_dir='./demonstrations')
    
    # Goal5æ¸è¿›å¼è¯¾ç¨‹
    goal5_center = np.array(config.goal5_center, dtype=np.float32)
    
    print(f"ğŸ“ Goal5æ¸è¿›å¼è¯¾ç¨‹:")
    print(f"  é˜¶æ®µ1 (0-25kæ­¥):   å›ºå®šGoal5")
    print(f"  é˜¶æ®µ2 (25k-50kæ­¥): å°æ‰°åŠ¨ Â± {config.goal5_radius_small}m")
    print(f"  é˜¶æ®µ3 (50k-75kæ­¥): ä¸­æ‰°åŠ¨ Â± {config.goal5_radius_medium}m")
    print(f"  é˜¶æ®µ4 (75k+æ­¥):    å¤§æ‰°åŠ¨ Â± {config.goal5_radius_large}m")
    print()
    
    pbar = tqdm(total=config.rl_total_timesteps, desc="Goal5 RLè®­ç»ƒ (BCæ­£åˆ™åŒ–)")
    
    # ä¸»è®­ç»ƒå¾ªç¯
    while total_timesteps < config.rl_total_timesteps:
        # æ¸è¿›å¼è¯¾ç¨‹
        if total_timesteps < 25000:
            curriculum_goal = goal5_center.copy()
            stage = "å›ºå®š"
        elif total_timesteps < 50000:
            noise = np.random.uniform(-config.goal5_radius_small, 
                                     config.goal5_radius_small, size=2)
            curriculum_goal = goal5_center + noise
            stage = "å°æ‰°åŠ¨"
        elif total_timesteps < 75000:
            noise = np.random.uniform(-config.goal5_radius_medium, 
                                     config.goal5_radius_medium, size=2)
            curriculum_goal = goal5_center + noise
            stage = "ä¸­æ‰°åŠ¨"
        else:
            noise = np.random.uniform(-config.goal5_radius_large, 
                                     config.goal5_radius_large, size=2)
            curriculum_goal = goal5_center + noise
            stage = "å¤§æ‰°åŠ¨"
        
        curriculum_goal = np.clip(
            curriculum_goal,
            [config.goal_space_bounds[0][0], config.goal_space_bounds[1][0]],
            [config.goal_space_bounds[0][1], config.goal_space_bounds[1][1]]
        )
        
        adaptive_max_steps = steps_manager.get_max_steps(curriculum_goal)
        
        env = ClearpathNavEnv(
            goal_pos=tuple(curriculum_goal), 
            max_steps=adaptive_max_steps
        )
        
        state, _ = env.reset()
        episode_reward = 0
        episode_steps = 0
        
        # Episodeå¾ªç¯
        for step in range(adaptive_max_steps):
            # é€‰æ‹©åŠ¨ä½œ
            if total_timesteps < config.rl_start_timesteps:
                action = env.action_space.sample()
            else:
                action = agent.select_action(state)
                # ğŸ”¥ é™ä½çš„æ¢ç´¢å™ªå£°
                noise = np.random.normal(0, config.max_action * config.expl_noise, 
                                        size=config.action_dim)
                action = (action + noise).clip(-config.max_action, config.max_action)
            
            next_state, reward, done, truncated, info = env.step(action)
            done_bool = float(done) if episode_steps < adaptive_max_steps else 0
            
            agent.store_transition(state, action, next_state, reward, done_bool)
            
            state = next_state
            episode_reward += reward
            episode_steps += 1
            total_timesteps += 1
            
            # ğŸ”¥ è®­ç»ƒç½‘ç»œ (å¸¦BCæ­£åˆ™åŒ–)
            if total_timesteps >= config.rl_start_timesteps:
                # æ ‡å‡†RLè®­ç»ƒ
                agent.train()
                
                # ğŸ”¥ BCæ­£åˆ™åŒ– (æ¯æ­¥éƒ½åº”ç”¨)
                if config.bc_regularization and len(bc_regularizer.demo_states) > 0:
                    # è®¡ç®—BCæŸå¤±
                    bc_loss = bc_regularizer.compute_bc_loss(
                        agent.actor, 
                        batch_size=config.bc_reg_batch_size
                    )
                    
                    # BCæ­£åˆ™åŒ–æ¢¯åº¦æ›´æ–°
                    agent.actor_optimizer.zero_grad()
                    (config.bc_reg_lambda * bc_loss).backward()
                    agent.actor_optimizer.step()
                    
                    bc_loss_history.append(bc_loss.item())
            
            # POLARéªŒè¯
            if total_timesteps > 0 and total_timesteps % config.verify_interval == 0:
                try:
                    is_safe, _ = agent.verify_safety(
                        state,
                        observation_error=config.observation_error,
                        bern_order=config.bern_order,
                        error_steps=config.error_steps
                    )
                    total_verify += 1
                    if is_safe:
                        safe_count += 1
                except:
                    pass
            
            pbar.update(1)
            
            if done or truncated:
                break
        
        # Episodeç»“æŸ
        episode_success = info.get('goal_reached', False)
        agent.update_capability(curriculum_goal, episode_success)
        steps_manager.update_online(curriculum_goal, episode_steps, episode_success)
        
        rewards_history.append(episode_reward)
        success_history.append(episode_success)
        goal_history.append(curriculum_goal.copy())
        stage_history.append(stage)
        
        distance_to_goal5 = np.linalg.norm(curriculum_goal - goal5_center)
        capability = agent.capability_estimator.estimate_capability(curriculum_goal)
        
        msg = (f"Ep {episode_num} [{stage:4s}]: "
               f"Goal=({curriculum_goal[0]:.2f},{curriculum_goal[1]:.2f}), "
               f"Dist2G5={distance_to_goal5:.2f}m, "
               f"R={episode_reward:.1f}, "
               f"Steps={episode_steps}/{adaptive_max_steps}, "
               f"Cap={capability:.1%}")
        
        if episode_success:
            msg += " ğŸ¯"
        elif info.get('collision'):
            msg += " âš ï¸"
        
        pbar.write(msg)
        
        episode_num += 1
        
        # å®šæœŸç»Ÿè®¡
        if episode_num % 50 == 0:
            recent_success = np.mean(success_history[-50:])
            avg_reward = np.mean(rewards_history[-50:])
            
            pbar.write("\n" + "="*70)
            pbar.write(f"Episode {episode_num} ç»Ÿè®¡:")
            pbar.write(f"  æœ€è¿‘50epæˆåŠŸç‡: {recent_success:.1%}")
            pbar.write(f"  æœ€è¿‘50epå¹³å‡å¥–åŠ±: {avg_reward:.1f}")
            
            # ğŸ”¥ BCæ­£åˆ™åŒ–æŸå¤±
            if len(bc_loss_history) > 0:
                recent_bc_loss = np.mean(bc_loss_history[-100:])
                pbar.write(f"  ğŸ”¥ BCæ­£åˆ™åŒ–æŸå¤±: {recent_bc_loss:.6f}")
            
            # å„é˜¶æ®µæˆåŠŸç‡
            recent_stages = stage_history[-50:]
            recent_successes = success_history[-50:]
            stage_stats = {}
            for s, succ in zip(recent_stages, recent_successes):
                if s not in stage_stats:
                    stage_stats[s] = {'success': 0, 'total': 0}
                stage_stats[s]['total'] += 1
                if succ:
                    stage_stats[s]['success'] += 1
            
            pbar.write(f"\n  å„é˜¶æ®µæˆåŠŸç‡:")
            for stage_name, stats in stage_stats.items():
                sr = stats['success'] / stats['total'] if stats['total'] > 0 else 0
                pbar.write(f"    {stage_name:6s}: {sr:.1%}")
            
            goal5_capability = agent.capability_estimator.estimate_capability(goal5_center)
            pbar.write(f"\n  ğŸ¯ Goal5ä¸­å¿ƒèƒ½åŠ›: {goal5_capability:.1%}")
            
            pbar.write("="*70 + "\n")
            
            agent.save_gcpo(f"{model_dir}/episode_{episode_num}")
            pbar.write(f"âœ“ æ¨¡å‹å·²ä¿å­˜\n")
        
        pbar.set_postfix({
            'Ep': episode_num,
            'Stage': stage,
            'Success': f'{np.mean(success_history[-20:]):.1%}' if len(success_history) >= 20 else 'N/A'
        })
        
        env.close()
    
    pbar.close()
    
    # è®­ç»ƒå®Œæˆç»Ÿè®¡
    print("\n" + "="*80)
    print("Goal5 è®­ç»ƒå®Œæˆ (å¸¦BCæ­£åˆ™åŒ–)!")
    print("="*80)
    print(f"æ€»Episodes: {episode_num}")
    print(f"æ€»Steps: {total_timesteps}")
    print(f"æ€»æˆåŠŸç‡: {np.mean(success_history):.1%}")
    print(f"æœ€è¿‘50epæˆåŠŸç‡: {np.mean(success_history[-50:]):.1%}")
    
    # BCæ­£åˆ™åŒ–æ•ˆæœ
    if len(bc_loss_history) > 0:
        print(f"\nğŸ”¥ BCæ­£åˆ™åŒ–ç»Ÿè®¡:")
        print(f"  å¹³å‡BCæŸå¤±: {np.mean(bc_loss_history):.6f}")
        print(f"  æœ€ç»ˆBCæŸå¤±: {np.mean(bc_loss_history[-100:]):.6f}")
    
    # å„é˜¶æ®µç»Ÿè®¡
    print(f"\nå„é˜¶æ®µæ•´ä½“ç»Ÿè®¡:")
    unique_stages = list(dict.fromkeys(stage_history))
    for stage_name in unique_stages:
        stage_indices = [i for i, s in enumerate(stage_history) if s == stage_name]
        stage_successes = [success_history[i] for i in stage_indices]
        if len(stage_successes) > 0:
            print(f"  {stage_name:6s}: {np.mean(stage_successes):.1%} "
                  f"({sum(stage_successes)}/{len(stage_successes)})")
    
    goal5_capability = agent.capability_estimator.estimate_capability(goal5_center)
    print(f"\nğŸ¯ Goal5ä¸­å¿ƒæœ€ç»ˆèƒ½åŠ›: {goal5_capability:.1%}")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_path = os.path.join(model_dir, "final")
    agent.save_gcpo(final_path)
    print(f"\nâœ“ æœ€ç»ˆæ¨¡å‹: {final_path}")
    
    # ä¿å­˜è®­ç»ƒæ•°æ®
    np.savez(
        f"{log_dir}/training_history.npz",
        rewards=rewards_history,
        successes=success_history,
        goals=np.array(goal_history),
        stages=stage_history,
        bc_losses=bc_loss_history if len(bc_loss_history) > 0 else []
    )
    
    return agent


if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='GCPO Goal5è®­ç»ƒ (å¸¦BCæ­£åˆ™åŒ–)')
    parser.add_argument('--bc_model', type=str, default='./models/bc_pretrained_goal5')
    parser.add_argument('--timesteps', type=int, default=100000)
    parser.add_argument('--bc_reg_lambda', type=float, default=0.1,
                       help='BCæ­£åˆ™åŒ–æƒé‡ (æ¨è0.05-0.5)')
    parser.add_argument('--save_dir', type=str, default='./models/gcpo_goal5_bc_reg')
    
    args = parser.parse_args()
    
    try:
        config = GCPOGoal5ConfigWithBCReg()
        config.bc_reg_lambda = args.bc_reg_lambda
        config.rl_total_timesteps = args.timesteps
        
        # åˆ›å»ºæ™ºèƒ½ä½“
        agent = GCPOAgent(12, 2, 0.5, config)
        
        # åŠ è½½BCæ¨¡å‹
        if os.path.exists(args.bc_model):
            print(f"åŠ è½½BCæ¨¡å‹: {args.bc_model}")
            agent.load(args.bc_model)
            start_from_bc = True
        else:
            print(f"âš ï¸  BCæ¨¡å‹ä¸å­˜åœ¨ï¼Œä»éšæœºåˆå§‹åŒ–å¼€å§‹")
            start_from_bc = False
        
        # ğŸ”¥ åˆ›å»ºBCæ­£åˆ™åŒ–å™¨
        bc_regularizer = BCRegularizer(
            demo_dir=config.bc_demo_dir,
            goal_idx=5,
            device=agent.device
        )
        
        # è®­ç»ƒ
        agent = train_gcpo_goal5_with_bc_reg(
            agent,
            config,
            bc_regularizer,
            start_from_bc=start_from_bc,
            save_dir=args.save_dir
        )
        
        print("\nâœ… å…¨éƒ¨å®Œæˆ!")
        
    except KeyboardInterrupt:
        print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()