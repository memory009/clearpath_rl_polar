#!/usr/bin/env python3
"""
Clearpath TD3+POLAR è®­ç»ƒè„šæœ¬
"""

import sys
import os
import numpy as np
import time
from datetime import datetime
from tqdm import tqdm

# æ·»åŠ è·¯å¾„
sys.path.append(os.path.dirname(__file__))

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from algorithms.td3_polar import TD3Agent
from envs.clearpath_nav_env import ClearpathNavEnv
from utils.config import TD3Config


def train_td3_polar(config=None):
    """
    ä¸»è®­ç»ƒå‡½æ•°
    
    Args:
        config: TD3Configå¯¹è±¡
    """
    if config is None:
        config = TD3Config()
    
    print("\n" + "="*80)
    print("Clearpath TD3+POLAR è®­ç»ƒ")
    print("="*80)
    
    # 1. åˆ›å»ºç¯å¢ƒ
    print("\n[1/4] åˆ›å»ºç¯å¢ƒ...")
    env = ClearpathNavEnv(
        robot_name=config.robot_name,
        goal_pos=config.goal_pos,
        max_steps=config.max_steps,
        collision_threshold=config.collision_threshold
    )
    print(f"âœ“ ç¯å¢ƒOK: è§‚æµ‹={env.observation_space.shape}, åŠ¨ä½œ={env.action_space.shape}")
    
    # 2. åˆ›å»ºæ™ºèƒ½ä½“
    print("\n[2/4] åˆ›å»ºTD3æ™ºèƒ½ä½“...")
    agent = TD3Agent(
        state_dim=config.state_dim,
        action_dim=config.action_dim,
        max_action=config.max_action,
        config=config
    )
    
    # 3. åˆ›å»ºæ—¥å¿—ç›®å½•
    print("\n[3/4] å‡†å¤‡æ—¥å¿—...")
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = os.path.join(config.log_path, timestamp)
    os.makedirs(log_dir, exist_ok=True)
    print(f"âœ“ æ—¥å¿—ç›®å½•: {log_dir}")
    
    # 4. è®­ç»ƒå¾ªç¯
    print("\n[4/4] å¼€å§‹è®­ç»ƒ...")
    print(f"  æ€»æ­¥æ•°: {config.total_timesteps}")
    print(f"  éšæœºæ¢ç´¢: {config.start_timesteps}æ­¥")
    print(f"  POLARéªŒè¯é—´éš”: {config.verify_interval}æ­¥")
    
    # ===== æ–°å¢ï¼šå¤šç›®æ ‡æ±  =====
    goal_pool = [
        (3.0, -2.0),   # åŸç›®æ ‡ï¼ˆå³ä¸Šï¼‰
        (2.0, 2.0),    # å·¦ä¸Š
        (-2.0, 2.0),   # å·¦ä¸‹
        (-2.0, -2.0),  # å³ä¸‹
        (3.0, 0.0),    # ä¸Šä¸­
        (0.0, 3.0),    # å·¦ä¸­
    ]
    print(f"\n  ğŸ¯ å¤šç›®æ ‡è®­ç»ƒæ¨¡å¼ï¼š{len(goal_pool)}ä¸ªç›®æ ‡ç‚¹")
    for i, goal in enumerate(goal_pool, 1):
        print(f"    ç›®æ ‡{i}: {goal}")
    
    # ç›®æ ‡ç»Ÿè®¡
    goal_stats = {goal: {'success': 0, 'collision': 0, 'timeout': 0, 'total': 0} 
                  for goal in goal_pool}
    current_goal = goal_pool[0]
    env.world_goal = np.array(current_goal, dtype=np.float32)
    
    state, _ = env.reset()
    episode_reward = 0
    episode_steps = 0
    episode_num = 0
    
    # ç»Ÿè®¡å˜é‡
    rewards_history = []
    safe_count = 0
    total_verify = 0
    
    pbar = tqdm(total=config.total_timesteps, desc="è®­ç»ƒè¿›åº¦")
    
    for t in range(config.total_timesteps):
        # é€‰æ‹©åŠ¨ä½œ
        if t < config.start_timesteps:
            action = env.action_space.sample()
        else:
            action = agent.select_action(state)
            noise = np.random.normal(0, config.max_action * config.expl_noise, 
                                    size=config.action_dim)
            action = (action + noise).clip(-config.max_action, config.max_action)
        
        # æ‰§è¡ŒåŠ¨ä½œ
        next_state, reward, done, truncated, info = env.step(action)
        done_bool = float(done) if episode_steps < config.max_steps else 0
        
        # å­˜å‚¨ç»éªŒ
        agent.store_transition(state, action, next_state, reward, done_bool)
        
        state = next_state
        episode_reward += reward
        episode_steps += 1
        
        # æ›´æ–°ç½‘ç»œ
        if t >= config.start_timesteps:
            agent.train()
        
        # POLARéªŒè¯
        if t > 0 and t % config.verify_interval == 0:
            try:
                is_safe, action_ranges = agent.verify_safety(
                    state,
                    observation_error=config.observation_error,
                    bern_order=config.bern_order,
                    error_steps=config.error_steps
                )
                
                total_verify += 1
                if is_safe:
                    safe_count += 1
                
                pbar.write(f"[éªŒè¯] æ­¥æ•°{t}: å®‰å…¨ç‡={safe_count}/{total_verify} ({safe_count/total_verify:.1%})")
            except Exception as e:
                pbar.write(f"[è­¦å‘Š] POLARéªŒè¯å¤±è´¥: {e}")
        
        # Episodeç»“æŸ
        if done or truncated:
            rewards_history.append(episode_reward)
            
            # ===== æ–°å¢ï¼šè®°å½•å½“å‰ç›®æ ‡çš„ç»Ÿè®¡ =====
            goal_stats[current_goal]['total'] += 1
            if info.get('goal_reached'):
                goal_stats[current_goal]['success'] += 1
            elif info.get('collision'):
                goal_stats[current_goal]['collision'] += 1
            elif info.get('timeout'):
                goal_stats[current_goal]['timeout'] += 1
            
            # æ„å»ºæ¶ˆæ¯
            msg = f"Episode {episode_num}: R={episode_reward:.1f}, Steps={episode_steps}, ç›®æ ‡{current_goal}"
            if info.get('goal_reached'):
                msg += " ğŸ¯æˆåŠŸ"
            elif info.get('collision'):
                msg += " âš ï¸ç¢°æ’"
            elif info.get('timeout'):
                msg += " â±ï¸è¶…æ—¶"
            pbar.write(msg)
            
            # ===== æ–°å¢ï¼šéšæœºé€‰æ‹©æ–°ç›®æ ‡ =====
            current_goal = goal_pool[np.random.randint(len(goal_pool))]
            env.world_goal = np.array(current_goal, dtype=np.float32)
            
            # é‡ç½®
            state, _ = env.reset()
            episode_reward = 0
            episode_steps = 0
            episode_num += 1
            
            # å®šæœŸä¿å­˜
            if episode_num % 50 == 0:
                model_path = os.path.join(config.model_path, timestamp)
                os.makedirs(model_path, exist_ok=True)
                agent.save(f"{model_path}/episode_{episode_num}")
                pbar.write(f"âœ“ æ¨¡å‹å·²ä¿å­˜: episode_{episode_num}")
                
                # ===== æ–°å¢ï¼šæ‰“å°ç›®æ ‡ç»Ÿè®¡ =====
                pbar.write("\n" + "="*70)
                pbar.write(f"Episode {episode_num} - å„ç›®æ ‡è¡¨ç°ç»Ÿè®¡:")
                pbar.write("="*70)
                for goal, stats in goal_stats.items():
                    total = stats['total']
                    if total > 0:
                        success_rate = stats['success'] / total * 100
                        collision_rate = stats['collision'] / total * 100
                        pbar.write(f"  {str(goal):<15s}: {total:3d}æ¬¡ | "
                                  f"æˆåŠŸ {success_rate:5.1f}% ({stats['success']:2d}) | "
                                  f"ç¢°æ’ {collision_rate:5.1f}% ({stats['collision']:2d})")
                pbar.write("="*70 + "\n")
        
        pbar.update(1)
        pbar.set_postfix({
            'Ep': episode_num,
            'R': f'{episode_reward:.0f}',
            'Safe': f'{safe_count}/{total_verify}'
        })
    
    pbar.close()
    
    # è®­ç»ƒå®Œæˆ
    print("\n" + "="*80)
    print("è®­ç»ƒå®Œæˆï¼")
    print("="*80)
    print(f"æ€»Episode: {episode_num}")
    print(f"å¹³å‡å¥–åŠ±: {np.mean(rewards_history):.2f}")
    print(f"æœ€é«˜å¥–åŠ±: {np.max(rewards_history):.2f}")
    if total_verify > 0:
        print(f"POLARå®‰å…¨ç‡: {safe_count/total_verify:.2%}")
    
    # ===== æ–°å¢ï¼šå„ç›®æ ‡æœ€ç»ˆè¡¨ç°ç»Ÿè®¡ =====
    print("\n" + "="*80)
    print("å„ç›®æ ‡æœ€ç»ˆè¡¨ç°:")
    print("="*80)
    print(f"{'ç›®æ ‡ä½ç½®':<15s} {'è®­ç»ƒæ¬¡æ•°':<10s} {'æˆåŠŸç‡':<12s} {'ç¢°æ’ç‡':<12s}")
    print("-" * 80)
    
    total_success = 0
    total_collision = 0
    total_episodes = 0
    
    for goal, stats in goal_stats.items():
        total = stats['total']
        if total > 0:
            success_rate = stats['success'] / total * 100
            collision_rate = stats['collision'] / total * 100
            print(f"{str(goal):<15s} {total:<10d} "
                  f"{success_rate:>5.1f}% ({stats['success']:3d}) "
                  f"{collision_rate:>5.1f}% ({stats['collision']:3d})")
            
            total_success += stats['success']
            total_collision += stats['collision']
            total_episodes += total
    
    # æ€»ä½“ç»Ÿè®¡
    print("-" * 80)
    if total_episodes > 0:
        overall_success = total_success / total_episodes * 100
        overall_collision = total_collision / total_episodes * 100
        print(f"{'æ€»ä½“':<15s} {total_episodes:<10d} "
              f"{overall_success:>5.1f}% ({total_success:3d}) "
              f"{overall_collision:>5.1f}% ({total_collision:3d})")
    print("="*80)
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_path = os.path.join(config.model_path, f"final_{timestamp}")
    os.makedirs(final_path, exist_ok=True)
    agent.save(final_path)
    agent.save_weights(f"{final_path}/weights_for_polar.npz")
    print(f"\nâœ“ æœ€ç»ˆæ¨¡å‹: {final_path}")
    print(f"âœ“ POLARæƒé‡: {final_path}/weights_for_polar.npz")
    
    env.close()
    
    return agent


if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser()
    parser.add_argument('--timesteps', type=int, default=100000)
    parser.add_argument('--start_timesteps', type=int, default=25000)
    parser.add_argument('--verify_interval', type=int, default=1000)
    
    args = parser.parse_args()
    
    config = TD3Config()
    config.total_timesteps = args.timesteps
    config.start_timesteps = args.start_timesteps
    config.verify_interval = args.verify_interval
    
    agent = train_td3_polar(config)
    
    print("\nâœ“ å…¨éƒ¨å®Œæˆï¼")