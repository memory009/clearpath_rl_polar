#!/usr/bin/env python3
"""
Behavioral Cloning é¢„è®­ç»ƒ - ä¸“é—¨é’ˆå¯¹ Goal5
ä»Goal5çš„äººå·¥æ¼”ç¤ºä¸­å­¦ä¹ åˆå§‹ç­–ç•¥
"""

import sys
import os
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from datetime import datetime
from tqdm import tqdm
import glob

sys.path.append(os.path.dirname(__file__))
from algorithms.td3_polar import TD3Agent
from algorithms.networks import Actor
from utils.config import TD3Config


class Goal5DemonstrationDataset(Dataset):
    """Goal5æ¼”ç¤ºæ•°æ®é›†"""
    
    def __init__(self, demo_dir, goal_idx=5, verbose=True):
        """
        åŠ è½½Goal5æ¼”ç¤ºæ•°æ®
        
        Args:
            demo_dir: æ¼”ç¤ºæ•°æ®ç›®å½•
            goal_idx: ç›®æ ‡ç´¢å¼• (é»˜è®¤5)
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        """
        self.states = []
        self.actions = []
        self.goals = []
        self.goal_idx = goal_idx
        
        # åŠ è½½Goal5çš„æ¼”ç¤ºæ–‡ä»¶
        # æ”¯æŒä¸¤ç§å‘½åæ ¼å¼:
        # 1. demo_goal5_num*.npz (æ¥è‡ª collect_goal5_supplement.py)
        # 2. demo_goal4_num*.npz (æ¥è‡ª collect_demonstrations.py, å¦‚æœgoal_idx=4)
        pattern = os.path.join(demo_dir, f'demo_goal{goal_idx}_num*.npz')
        demo_files = sorted(glob.glob(pattern))
        
        if len(demo_files) == 0:
            raise ValueError(f"åœ¨ {demo_dir} ä¸­æ²¡æœ‰æ‰¾åˆ° Goal{goal_idx} çš„æ¼”ç¤ºæ•°æ®æ–‡ä»¶!\n"
                           f"  æŸ¥æ‰¾æ¨¡å¼: {pattern}\n"
                           f"  è¯·ç¡®è®¤:\n"
                           f"    1. ç›®å½•è·¯å¾„æ­£ç¡®\n"
                           f"    2. å·²ç»æ”¶é›†äº†Goal{goal_idx}çš„æ¼”ç¤ºæ•°æ®\n"
                           f"    3. æ–‡ä»¶å‘½åæ ¼å¼ä¸º demo_goal{goal_idx}_num*.npz")
        
        if verbose:
            print(f"\n{'='*70}")
            print(f"åŠ è½½ Goal{goal_idx} æ¼”ç¤ºæ•°æ®")
            print(f"{'='*70}")
            print(f"  ç›®å½•: {demo_dir}")
            print(f"  æ–‡ä»¶æ•°: {len(demo_files)}")
        
        total_steps = 0
        goal_positions = []
        
        for demo_file in demo_files:
            data = np.load(demo_file)
            
            states = data['states']
            actions = data['actions']
            goals = data['goals']
            
            # éªŒè¯goal_idx
            file_goal_idx = data.get('goal_idx', None)
            if file_goal_idx is not None and file_goal_idx != goal_idx:
                if verbose:
                    print(f"  âš ï¸  è·³è¿‡æ–‡ä»¶ (goal_idxä¸åŒ¹é…): {os.path.basename(demo_file)}")
                continue
            
            # æ·»åŠ åˆ°æ•°æ®é›†
            self.states.extend(states)
            self.actions.extend(actions)
            self.goals.extend(goals)
            
            total_steps += len(states)
            goal_positions.append(goals[0])
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        self.states = np.array(self.states, dtype=np.float32)
        self.actions = np.array(self.actions, dtype=np.float32)
        self.goals = np.array(self.goals, dtype=np.float32)
        
        if verbose:
            print(f"  æ€»episodes: {len(demo_files)}")
            print(f"  æ€»æ­¥æ•°: {total_steps}")
            print(f"  å¹³å‡æ­¥æ•°/episode: {total_steps/len(demo_files):.1f}")
            
            # æ˜¾ç¤ºç›®æ ‡ä½ç½®ç»Ÿè®¡
            if len(goal_positions) > 0:
                goal_mean = np.mean(goal_positions, axis=0)
                goal_std = np.std(goal_positions, axis=0)
                print(f"\n  Goal{goal_idx} ç›®æ ‡ä½ç½®ç»Ÿè®¡:")
                print(f"    å¹³å‡: ({goal_mean[0]:.2f}, {goal_mean[1]:.2f})")
                print(f"    æ ‡å‡†å·®: ({goal_std[0]:.3f}, {goal_std[1]:.3f})")
            
            print(f"{'='*70}\n")
    
    def __len__(self):
        return len(self.states)
    
    def __getitem__(self, idx):
        """
        è·å–ä¸€ä¸ªæ ·æœ¬
        
        Returns:
            state: è§‚æµ‹çŠ¶æ€
            action: å¯¹åº”çš„åŠ¨ä½œ
            goal: ç›®æ ‡ä½ç½®
        """
        return (
            torch.FloatTensor(self.states[idx]),
            torch.FloatTensor(self.actions[idx]),
            torch.FloatTensor(self.goals[idx])
        )


class BCTrainer:
    """è¡Œä¸ºå…‹éš†è®­ç»ƒå™¨"""
    
    def __init__(self, agent, config):
        """
        åˆå§‹åŒ–BCè®­ç»ƒå™¨
        
        Args:
            agent: TD3Agentå¯¹è±¡
            config: è®­ç»ƒé…ç½®
        """
        self.agent = agent
        self.config = config
        self.device = agent.device
        
        # BCä¸“ç”¨çš„ä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(
            agent.actor.parameters(), 
            lr=config.bc_learning_rate
        )
        
        # æŸå¤±å‡½æ•°
        self.criterion = nn.MSELoss()
        
        # è®­ç»ƒç»Ÿè®¡
        self.train_losses = []
        self.val_losses = []
        
        print(f"âœ“ BCè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"  å­¦ä¹ ç‡: {config.bc_learning_rate}")
        print(f"  Batchå¤§å°: {config.bc_batch_size}")
    
    def train_epoch(self, train_loader):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.agent.actor.train()
        
        epoch_loss = 0.0
        num_batches = 0
        
        for states, actions, goals in train_loader:
            states = states.to(self.device)
            actions = actions.to(self.device)
            
            # å‰å‘ä¼ æ’­
            predicted_actions = self.agent.actor(states)
            
            # è®¡ç®—æŸå¤±
            loss = self.criterion(predicted_actions, actions)
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(
                self.agent.actor.parameters(), 
                max_norm=1.0
            )
            
            self.optimizer.step()
            
            epoch_loss += loss.item()
            num_batches += 1
        
        avg_loss = epoch_loss / num_batches
        return avg_loss
    
    def validate(self, val_loader):
        """éªŒè¯"""
        self.agent.actor.eval()
        
        val_loss = 0.0
        num_batches = 0
        
        with torch.no_grad():
            for states, actions, goals in val_loader:
                states = states.to(self.device)
                actions = actions.to(self.device)
                
                predicted_actions = self.agent.actor(states)
                loss = self.criterion(predicted_actions, actions)
                
                val_loss += loss.item()
                num_batches += 1
        
        avg_loss = val_loss / num_batches
        return avg_loss
    
    def train(self, train_loader, val_loader, num_epochs):
        """
        å®Œæ•´è®­ç»ƒæµç¨‹
        
        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            num_epochs: è®­ç»ƒè½®æ•°
        
        Returns:
            best_val_loss: æœ€ä½³éªŒè¯æŸå¤±
        """
        best_val_loss = float('inf')
        patience_counter = 0
        patience = 10  # Early stoppingè€å¿ƒå€¼
        
        print(f"\n{'='*70}")
        print(f"å¼€å§‹BCè®­ç»ƒ - Goal5ä¸“ç”¨")
        print(f"{'='*70}")
        print(f"  Epochs: {num_epochs}")
        print(f"  è®­ç»ƒæ ·æœ¬: {len(train_loader.dataset)}")
        print(f"  éªŒè¯æ ·æœ¬: {len(val_loader.dataset)}")
        print(f"{'='*70}\n")
        
        for epoch in range(num_epochs):
            # è®­ç»ƒ
            train_loss = self.train_epoch(train_loader)
            self.train_losses.append(train_loss)
            
            # éªŒè¯
            val_loss = self.validate(val_loader)
            self.val_losses.append(val_loss)
            
            # æ‰“å°è¿›åº¦
            print(f"Epoch {epoch+1:3d}/{num_epochs} | "
                  f"Train Loss: {train_loss:.6f} | "
                  f"Val Loss: {val_loss:.6f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                print(f"  âœ“ æ–°çš„æœ€ä½³æ¨¡å‹ (val_loss={val_loss:.6f})")
            else:
                patience_counter += 1
            
            # Early stopping
            if patience_counter >= patience:
                print(f"\nâš ï¸  éªŒè¯æŸå¤±åœ¨{patience}ä¸ªepochå†…æœªæ”¹å–„,æå‰åœæ­¢")
                break
        
        print(f"\n{'='*70}")
        print(f"BCè®­ç»ƒå®Œæˆ")
        print(f"{'='*70}")
        print(f"  æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
        print(f"  æœ€ç»ˆè®­ç»ƒæŸå¤±: {self.train_losses[-1]:.6f}")
        print(f"{'='*70}\n")
        
        return best_val_loss


def pretrain_goal5_from_demonstrations(
    demo_dir='./demonstrations_goal5',
    model_save_path='./models/bc_pretrained_goal5',
    goal_idx=5,
    num_epochs=100,
    batch_size=256,
    learning_rate=3e-4,
    val_split=0.1,
    seed=42
):
    """
    ä»Goal5æ¼”ç¤ºæ•°æ®è¿›è¡ŒBCé¢„è®­ç»ƒçš„ä¸»å‡½æ•°
    
    Args:
        demo_dir: Goal5æ¼”ç¤ºæ•°æ®ç›®å½•
        model_save_path: æ¨¡å‹ä¿å­˜è·¯å¾„
        goal_idx: ç›®æ ‡ç´¢å¼• (é»˜è®¤5)
        num_epochs: è®­ç»ƒè½®æ•°
        batch_size: Batchå¤§å°
        learning_rate: å­¦ä¹ ç‡
        val_split: éªŒè¯é›†æ¯”ä¾‹
        seed: éšæœºç§å­
    
    Returns:
        agent: é¢„è®­ç»ƒåçš„æ™ºèƒ½ä½“
        best_val_loss: æœ€ä½³éªŒè¯æŸå¤±
    """
    print("\n" + "="*70)
    print("ğŸ¯ Behavioral Cloning é¢„è®­ç»ƒ - Goal5 ä¸“ç”¨")
    print("="*70)
    
    # è®¾ç½®éšæœºç§å­
    np.random.seed(seed)
    torch.manual_seed(seed)
    
    # 1. åŠ è½½Goal5æ¼”ç¤ºæ•°æ®
    dataset = Goal5DemonstrationDataset(demo_dir, goal_idx=goal_idx, verbose=True)
    
    # 2. åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    num_val = int(len(dataset) * val_split)
    num_train = len(dataset) - num_val
    
    train_dataset, val_dataset = torch.utils.data.random_split(
        dataset, 
        [num_train, num_val],
        generator=torch.Generator().manual_seed(seed)
    )
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=2
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=2
    )
    
    print(f"æ•°æ®åˆ’åˆ†:")
    print(f"  è®­ç»ƒé›†: {num_train} æ ·æœ¬")
    print(f"  éªŒè¯é›†: {num_val} æ ·æœ¬")
    
    # 3. åˆ›å»ºæ™ºèƒ½ä½“
    config = TD3Config()
    config.bc_learning_rate = learning_rate
    config.bc_batch_size = batch_size
    
    agent = TD3Agent(
        state_dim=12,
        action_dim=2,
        max_action=0.5,
        config=config
    )
    
    # 4. åˆ›å»ºBCè®­ç»ƒå™¨
    trainer = BCTrainer(agent, config)
    
    # 5. è®­ç»ƒ
    best_val_loss = trainer.train(train_loader, val_loader, num_epochs)
    
    # 6. ä¿å­˜æ¨¡å‹
    os.makedirs(model_save_path, exist_ok=True)
    agent.save(model_save_path)
    
    # ä¿å­˜è®­ç»ƒæ›²çº¿
    np.savez(
        os.path.join(model_save_path, 'bc_training_curves_goal5.npz'),
        train_losses=trainer.train_losses,
        val_losses=trainer.val_losses,
        goal_idx=goal_idx
    )
    
    # ä¿å­˜è®­ç»ƒä¿¡æ¯
    with open(os.path.join(model_save_path, 'training_info.txt'), 'w') as f:
        f.write(f"Goal5 BC Pretraining\n")
        f.write(f"{'='*50}\n")
        f.write(f"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"Demo Directory: {demo_dir}\n")
        f.write(f"Goal Index: {goal_idx}\n")
        f.write(f"Total Samples: {len(dataset)}\n")
        f.write(f"Training Samples: {num_train}\n")
        f.write(f"Validation Samples: {num_val}\n")
        f.write(f"Epochs: {num_epochs}\n")
        f.write(f"Batch Size: {batch_size}\n")
        f.write(f"Learning Rate: {learning_rate}\n")
        f.write(f"Best Val Loss: {best_val_loss:.6f}\n")
        f.write(f"Final Train Loss: {trainer.train_losses[-1]:.6f}\n")
    
    print(f"\n{'='*70}")
    print(f"âœ… Goal5 é¢„è®­ç»ƒå®Œæˆ")
    print(f"{'='*70}")
    print(f"  æ¨¡å‹ä¿å­˜: {model_save_path}")
    print(f"  æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
    print(f"  è®­ç»ƒä¿¡æ¯: {model_save_path}/training_info.txt")
    print(f"{'='*70}\n")
    
    return agent, best_val_loss


def evaluate_goal5_bc_policy(agent, env, num_episodes=10):
    """
    è¯„ä¼°Goal5 BCé¢„è®­ç»ƒçš„ç­–ç•¥
    
    Args:
        agent: é¢„è®­ç»ƒçš„æ™ºèƒ½ä½“
        env: ç¯å¢ƒ
        num_episodes: æµ‹è¯•episodeæ•°
    
    Returns:
        results: è¯„ä¼°ç»“æœå­—å…¸
    """
    print(f"\n{'='*70}")
    print(f"è¯„ä¼° Goal5 BCé¢„è®­ç»ƒç­–ç•¥")
    print(f"{'='*70}")

    max_steps = env.max_steps
    print(f"è¯„ä¼°æ—¶çš„æœ€å¤§æ­¥æ•°: {max_steps}")
    print(f"ç›®æ ‡ä½ç½®: ({env.world_goal[0]:.2f}, {env.world_goal[1]:.2f})")
    
    success_count = 0
    total_rewards = []
    total_steps = []
    final_distances = []
    collision_count = 0
    timeout_count = 0
    
    for ep in range(num_episodes):
        state, _ = env.reset()
        episode_reward = 0
        episode_steps = 0
        
        done = False
        while not done and episode_steps < max_steps:
            # ä½¿ç”¨ç­–ç•¥é€‰æ‹©åŠ¨ä½œ(æ— å™ªå£°)
            action = agent.select_action(state)
            
            # æ‰§è¡Œ
            next_state, reward, done, truncated, info = env.step(action)
            
            episode_reward += reward
            episode_steps += 1
            state = next_state
            
            if done or truncated:
                break
        
        # ç»Ÿè®¡
        success = info.get('goal_reached', False)
        collision = info.get('collision', False)
        
        if success:
            success_count += 1
        if collision:
            collision_count += 1
        if episode_steps >= max_steps:
            timeout_count += 1
        
        total_rewards.append(episode_reward)
        total_steps.append(episode_steps)
        final_distances.append(info.get('distance', 0))
        
        status_icon = 'âœ…' if success else ('ğŸ’¥' if collision else 'â±ï¸')
        print(f"  Episode {ep+1:2d}: "
              f"Reward={episode_reward:7.1f}, "
              f"Steps={episode_steps:3d}, "
              f"Dist={final_distances[-1]:5.2f}m "
              f"{status_icon}")
    
    # ç»“æœ
    results = {
        'success_rate': success_count / num_episodes,
        'collision_rate': collision_count / num_episodes,
        'timeout_rate': timeout_count / num_episodes,
        'avg_reward': np.mean(total_rewards),
        'avg_steps': np.mean(total_steps),
        'avg_final_distance': np.mean(final_distances),
        'min_distance': np.min(final_distances),
        'max_distance': np.max(final_distances)
    }
    
    print(f"\n{'='*70}")
    print(f"Goal5 è¯„ä¼°ç»“æœ:")
    print(f"{'='*70}")
    print(f"  æˆåŠŸç‡: {results['success_rate']:.1%} ({success_count}/{num_episodes})")
    print(f"  ç¢°æ’ç‡: {results['collision_rate']:.1%}")
    print(f"  è¶…æ—¶ç‡: {results['timeout_rate']:.1%}")
    print(f"  å¹³å‡å¥–åŠ±: {results['avg_reward']:.1f}")
    print(f"  å¹³å‡æ­¥æ•°: {results['avg_steps']:.1f}")
    print(f"  æœ€ç»ˆè·ç¦»: {results['avg_final_distance']:.2f}m (min={results['min_distance']:.2f}, max={results['max_distance']:.2f})")
    print(f"{'='*70}\n")
    
    return results


if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='Goal5 BCé¢„è®­ç»ƒ')
    parser.add_argument('--demo_dir', type=str, default='./demonstrations_goal5',
                       help='Goal5æ¼”ç¤ºæ•°æ®ç›®å½•')
    parser.add_argument('--save_path', type=str, default='./models/bc_pretrained_goal5',
                       help='æ¨¡å‹ä¿å­˜è·¯å¾„')
    parser.add_argument('--goal_idx', type=int, default=5,
                       help='ç›®æ ‡ç´¢å¼• (é»˜è®¤5)')
    parser.add_argument('--epochs', type=int, default=100,
                       help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=256,
                       help='Batchå¤§å°')
    parser.add_argument('--lr', type=float, default=3e-4,
                       help='å­¦ä¹ ç‡')
    parser.add_argument('--val_split', type=float, default=0.1,
                       help='éªŒè¯é›†æ¯”ä¾‹')
    parser.add_argument('--evaluate', action='store_true',
                       help='è®­ç»ƒåè¯„ä¼°ç­–ç•¥')
    parser.add_argument('--eval_goal', type=str, default='4.5,-2.0',
                       help='è¯„ä¼°ç›®æ ‡ä½ç½® (é»˜è®¤Goal5ä½ç½®)')
    parser.add_argument('--eval_max_steps', type=int, default=1024,
                       help='è¯„ä¼°æ—¶çš„æœ€å¤§æ­¥æ•°')
    parser.add_argument('--eval_episodes', type=int, default=10,
                       help='è¯„ä¼°episodeæ•°')
    
    args = parser.parse_args()
    
    try:
        # Goal5 BCé¢„è®­ç»ƒ
        agent, best_val_loss = pretrain_goal5_from_demonstrations(
            demo_dir=args.demo_dir,
            model_save_path=args.save_path,
            goal_idx=args.goal_idx,
            num_epochs=args.epochs,
            batch_size=args.batch_size,
            learning_rate=args.lr,
            val_split=args.val_split
        )
        
        # å¯é€‰ï¼šè¯„ä¼°ç­–ç•¥
        if args.evaluate:
            print("\nå¼€å§‹è¯„ä¼°Goal5é¢„è®­ç»ƒç­–ç•¥...")
            
            from envs.clearpath_nav_env import ClearpathNavEnv
            import rclpy
            
            if not rclpy.ok():
                rclpy.init()
            
            eval_goal = tuple(map(float, args.eval_goal.split(',')))
            env = ClearpathNavEnv(goal_pos=eval_goal, max_steps=args.eval_max_steps)
            
            results = evaluate_goal5_bc_policy(agent, env, num_episodes=args.eval_episodes)
            
            # ä¿å­˜è¯„ä¼°ç»“æœ
            eval_results_path = os.path.join(args.save_path, 'evaluation_results.npz')
            np.savez(eval_results_path, **results)
            print(f"è¯„ä¼°ç»“æœå·²ä¿å­˜: {eval_results_path}")
            
            env.close()
        
        print("\nâœ… å…¨éƒ¨å®Œæˆ!")
        
    except KeyboardInterrupt:
        print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()