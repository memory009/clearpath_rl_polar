#!/usr/bin/env python3
"""
Behavioral Cloning é¢„è®­ç»ƒ
ä»äººå·¥æ¼”ç¤ºä¸­å­¦ä¹ åˆå§‹ç­–ç•¥
"""

import sys
import os
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from datetime import datetime
from tqdm import tqdm
import glob

sys.path.append(os.path.dirname(__file__))
from algorithms.td3_polar import TD3Agent
from algorithms.networks import Actor
from utils.config import TD3Config


class DemonstrationDataset(Dataset):
    """æ¼”ç¤ºæ•°æ®é›†"""
    
    def __init__(self, demo_dir, verbose=True):
        """
        åŠ è½½æ¼”ç¤ºæ•°æ®
        
        Args:
            demo_dir: æ¼”ç¤ºæ•°æ®ç›®å½•
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        """
        self.states = []
        self.actions = []
        self.goals = []
        
        # åŠ è½½æ‰€æœ‰æ¼”ç¤ºæ–‡ä»¶
        demo_files = sorted(glob.glob(os.path.join(demo_dir, '*.npz')))
        
        if len(demo_files) == 0:
            raise ValueError(f"åœ¨ {demo_dir} ä¸­æ²¡æœ‰æ‰¾åˆ°æ¼”ç¤ºæ•°æ®æ–‡ä»¶!")
        
        if verbose:
            print(f"\n{'='*60}")
            print(f"åŠ è½½æ¼”ç¤ºæ•°æ®")
            print(f"{'='*60}")
            print(f"  ç›®å½•: {demo_dir}")
            print(f"  æ–‡ä»¶æ•°: {len(demo_files)}")
        
        total_steps = 0
        goal_distribution = {}
        
        for demo_file in demo_files:
            data = np.load(demo_file)
            
            states = data['states']
            actions = data['actions']
            goals = data['goals']
            
            # æ·»åŠ åˆ°æ•°æ®é›†
            self.states.extend(states)
            self.actions.extend(actions)
            self.goals.extend(goals)
            
            total_steps += len(states)
            
            # ç»Ÿè®¡ç›®æ ‡åˆ†å¸ƒ
            goal_key = tuple(np.round(goals[0], 1))
            goal_distribution[goal_key] = goal_distribution.get(goal_key, 0) + 1
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        self.states = np.array(self.states, dtype=np.float32)
        self.actions = np.array(self.actions, dtype=np.float32)
        self.goals = np.array(self.goals, dtype=np.float32)
        
        if verbose:
            print(f"  æ€»episodes: {len(demo_files)}")
            print(f"  æ€»æ­¥æ•°: {total_steps}")
            print(f"  å¹³å‡æ­¥æ•°/episode: {total_steps/len(demo_files):.1f}")
            print(f"\n  ç›®æ ‡åˆ†å¸ƒ:")
            for goal_pos, count in sorted(goal_distribution.items()):
                print(f"    {goal_pos}: {count} episodes")
            print(f"{'='*60}\n")
    
    def __len__(self):
        return len(self.states)
    
    def __getitem__(self, idx):
        """
        è·å–ä¸€ä¸ªæ ·æœ¬
        
        Returns:
            state: è§‚æµ‹çŠ¶æ€
            action: å¯¹åº”çš„åŠ¨ä½œ
            goal: ç›®æ ‡ä½ç½®
        """
        return (
            torch.FloatTensor(self.states[idx]),
            torch.FloatTensor(self.actions[idx]),
            torch.FloatTensor(self.goals[idx])
        )


class BCTrainer:
    """è¡Œä¸ºå…‹éš†è®­ç»ƒå™¨"""
    
    def __init__(self, agent, config):
        """
        åˆå§‹åŒ–BCè®­ç»ƒå™¨
        
        Args:
            agent: TD3Agentå¯¹è±¡
            config: è®­ç»ƒé…ç½®
        """
        self.agent = agent
        self.config = config
        self.device = agent.device
        
        # BCä¸“ç”¨çš„ä¼˜åŒ–å™¨(å­¦ä¹ ç‡å¯èƒ½ä¸åŒ)
        self.optimizer = optim.Adam(
            agent.actor.parameters(), 
            lr=config.bc_learning_rate
        )
        
        # æŸå¤±å‡½æ•°
        self.criterion = nn.MSELoss()
        
        # è®­ç»ƒç»Ÿè®¡
        self.train_losses = []
        self.val_losses = []
        
        print(f"âœ“ BCè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"  å­¦ä¹ ç‡: {config.bc_learning_rate}")
        print(f"  Batchå¤§å°: {config.bc_batch_size}")
    
    def train_epoch(self, train_loader):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.agent.actor.train()
        
        epoch_loss = 0.0
        num_batches = 0
        
        for states, actions, goals in train_loader:
            states = states.to(self.device)
            actions = actions.to(self.device)
            
            # å‰å‘ä¼ æ’­
            predicted_actions = self.agent.actor(states)
            
            # è®¡ç®—æŸå¤±
            loss = self.criterion(predicted_actions, actions)
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª(é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸)
            torch.nn.utils.clip_grad_norm_(
                self.agent.actor.parameters(), 
                max_norm=1.0
            )
            
            self.optimizer.step()
            
            epoch_loss += loss.item()
            num_batches += 1
        
        avg_loss = epoch_loss / num_batches
        return avg_loss
    
    def validate(self, val_loader):
        """éªŒè¯"""
        self.agent.actor.eval()
        
        val_loss = 0.0
        num_batches = 0
        
        with torch.no_grad():
            for states, actions, goals in val_loader:
                states = states.to(self.device)
                actions = actions.to(self.device)
                
                predicted_actions = self.agent.actor(states)
                loss = self.criterion(predicted_actions, actions)
                
                val_loss += loss.item()
                num_batches += 1
        
        avg_loss = val_loss / num_batches
        return avg_loss
    
    def train(self, train_loader, val_loader, num_epochs):
        """
        å®Œæ•´è®­ç»ƒæµç¨‹
        
        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            num_epochs: è®­ç»ƒè½®æ•°
        
        Returns:
            best_val_loss: æœ€ä½³éªŒè¯æŸå¤±
        """
        best_val_loss = float('inf')
        patience_counter = 0
        patience = 10  # Early stoppingè€å¿ƒå€¼
        
        print(f"\n{'='*60}")
        print(f"å¼€å§‹BCè®­ç»ƒ")
        print(f"{'='*60}")
        print(f"  Epochs: {num_epochs}")
        print(f"  è®­ç»ƒæ ·æœ¬: {len(train_loader.dataset)}")
        print(f"  éªŒè¯æ ·æœ¬: {len(val_loader.dataset)}")
        print(f"{'='*60}\n")
        
        for epoch in range(num_epochs):
            # è®­ç»ƒ
            train_loss = self.train_epoch(train_loader)
            self.train_losses.append(train_loss)
            
            # éªŒè¯
            val_loss = self.validate(val_loader)
            self.val_losses.append(val_loss)
            
            # æ‰“å°è¿›åº¦
            print(f"Epoch {epoch+1:3d}/{num_epochs} | "
                  f"Train Loss: {train_loss:.6f} | "
                  f"Val Loss: {val_loss:.6f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                print(f"  âœ“ æ–°çš„æœ€ä½³æ¨¡å‹ (val_loss={val_loss:.6f})")
            else:
                patience_counter += 1
            
            # Early stopping
            if patience_counter >= patience:
                print(f"\nâš ï¸  éªŒè¯æŸå¤±åœ¨{patience}ä¸ªepochå†…æœªæ”¹å–„,æå‰åœæ­¢")
                break
        
        print(f"\n{'='*60}")
        print(f"BCè®­ç»ƒå®Œæˆ")
        print(f"{'='*60}")
        if num_epochs > 0 and len(self.train_losses) > 0:
            print(f"  æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
            print(f"  æœ€ç»ˆè®­ç»ƒæŸå¤±: {self.train_losses[-1]:.6f}")
        else:
            print(f"  âš ï¸  æœªè¿›è¡Œè®­ç»ƒ (epochs=0)")
        print(f"{'='*60}\n")
        
        return best_val_loss


def pretrain_from_demonstrations(
    demo_dir='./demonstrations',
    model_save_path='./models/bc_pretrained',
    num_epochs=100,
    batch_size=256,
    learning_rate=3e-4,
    val_split=0.1,
    seed=42
):
    """
    ä»æ¼”ç¤ºæ•°æ®è¿›è¡ŒBCé¢„è®­ç»ƒçš„ä¸»å‡½æ•°
    
    Args:
        demo_dir: æ¼”ç¤ºæ•°æ®ç›®å½•
        model_save_path: æ¨¡å‹ä¿å­˜è·¯å¾„
        num_epochs: è®­ç»ƒè½®æ•°
        batch_size: Batchå¤§å°
        learning_rate: å­¦ä¹ ç‡
        val_split: éªŒè¯é›†æ¯”ä¾‹
        seed: éšæœºç§å­
    
    Returns:
        agent: é¢„è®­ç»ƒåçš„æ™ºèƒ½ä½“
        best_val_loss: æœ€ä½³éªŒè¯æŸå¤±
    """
    print("\n" + "="*70)
    print("ğŸ“ Behavioral Cloning é¢„è®­ç»ƒ")
    print("="*70)
    
    # è®¾ç½®éšæœºç§å­
    np.random.seed(seed)
    torch.manual_seed(seed)
    
    # 1. åŠ è½½æ¼”ç¤ºæ•°æ®
    dataset = DemonstrationDataset(demo_dir, verbose=True)
    
    # 2. åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    num_val = int(len(dataset) * val_split)
    num_train = len(dataset) - num_val
    
    train_dataset, val_dataset = torch.utils.data.random_split(
        dataset, 
        [num_train, num_val],
        generator=torch.Generator().manual_seed(seed)
    )
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=2
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=2
    )
    
    print(f"æ•°æ®åˆ’åˆ†:")
    print(f"  è®­ç»ƒé›†: {num_train} æ ·æœ¬")
    print(f"  éªŒè¯é›†: {num_val} æ ·æœ¬")
    
    # 3. åˆ›å»ºæ™ºèƒ½ä½“
    config = TD3Config()
    config.bc_learning_rate = learning_rate
    config.bc_batch_size = batch_size
    
    agent = TD3Agent(
        state_dim=12,
        action_dim=2,
        max_action=0.5,
        config=config
    )
    
    # 4. åˆ›å»ºBCè®­ç»ƒå™¨
    trainer = BCTrainer(agent, config)
    
    # 5. è®­ç»ƒ
    best_val_loss = trainer.train(train_loader, val_loader, num_epochs)
    
    # 6. ä¿å­˜æ¨¡å‹
    os.makedirs(model_save_path, exist_ok=True)
    agent.save(model_save_path)
    
    # ä¿å­˜è®­ç»ƒæ›²çº¿
    np.savez(
        os.path.join(model_save_path, 'bc_training_curves.npz'),
        train_losses=trainer.train_losses,
        val_losses=trainer.val_losses
    )
    
    print(f"\n{'='*70}")
    print(f"âœ… é¢„è®­ç»ƒå®Œæˆ")
    print(f"{'='*70}")
    print(f"  æ¨¡å‹ä¿å­˜: {model_save_path}")
    print(f"  æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
    print(f"{'='*70}\n")
    
    return agent, best_val_loss


def evaluate_bc_policy(agent, env, num_episodes=10):
    """
    è¯„ä¼°BCé¢„è®­ç»ƒçš„ç­–ç•¥
    
    Args:
        agent: é¢„è®­ç»ƒçš„æ™ºèƒ½ä½“
        env: ç¯å¢ƒ
        num_episodes: æµ‹è¯•episodeæ•°
    
    Returns:
        results: è¯„ä¼°ç»“æœå­—å…¸
    """
    print(f"\n{'='*60}")
    print(f"è¯„ä¼°BCé¢„è®­ç»ƒç­–ç•¥")
    print(f"{'='*60}")

    max_steps = env.max_steps
    print(f"è¯„ä¼°æ—¶çš„æœ€å¤§æ­¥æ•°: {max_steps}")
    
    success_count = 0
    total_rewards = []
    total_steps = []
    final_distances = []
    
    for ep in range(num_episodes):
        state, _ = env.reset()
        episode_reward = 0
        episode_steps = 0
        
        done = False
        while not done and episode_steps < max_steps:
            # ä½¿ç”¨ç­–ç•¥é€‰æ‹©åŠ¨ä½œ(æ— å™ªå£°)
            action = agent.select_action(state)
            
            # æ‰§è¡Œ
            next_state, reward, done, truncated, info = env.step(action)
            
            episode_reward += reward
            episode_steps += 1
            state = next_state
            
            if done or truncated:
                break
        
        # ç»Ÿè®¡
        success = info.get('goal_reached', False)
        if success:
            success_count += 1
        
        total_rewards.append(episode_reward)
        total_steps.append(episode_steps)
        final_distances.append(info.get('distance', 0))
        
        print(f"  Episode {ep+1:2d}: "
              f"Reward={episode_reward:6.1f}, "
              f"Steps={episode_steps:3d}, "
              f"Success={'âœ…' if success else 'âŒ'}")
    
    # ç»“æœ
    results = {
        'success_rate': success_count / num_episodes,
        'avg_reward': np.mean(total_rewards),
        'avg_steps': np.mean(total_steps),
        'avg_final_distance': np.mean(final_distances)
    }
    
    print(f"\n{'='*60}")
    print(f"è¯„ä¼°ç»“æœ:")
    print(f"{'='*60}")
    print(f"  æˆåŠŸç‡: {results['success_rate']:.1%}")
    print(f"  å¹³å‡å¥–åŠ±: {results['avg_reward']:.1f}")
    print(f"  å¹³å‡æ­¥æ•°: {results['avg_steps']:.1f}")
    print(f"  å¹³å‡æœ€ç»ˆè·ç¦»: {results['avg_final_distance']:.2f}m")
    print(f"{'='*60}\n")
    
    return results


if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='BCé¢„è®­ç»ƒ')
    parser.add_argument('--demo_dir', type=str, default='./demonstrations',
                       help='æ¼”ç¤ºæ•°æ®ç›®å½•')
    parser.add_argument('--save_path', type=str, default='./models/bc_pretrained',
                       help='æ¨¡å‹ä¿å­˜è·¯å¾„')
    parser.add_argument('--epochs', type=int, default=100,
                       help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=256,
                       help='Batchå¤§å°')
    parser.add_argument('--lr', type=float, default=3e-4,
                       help='å­¦ä¹ ç‡')
    parser.add_argument('--val_split', type=float, default=0.1,
                       help='éªŒè¯é›†æ¯”ä¾‹')
    parser.add_argument('--evaluate', action='store_true',
                       help='è®­ç»ƒåè¯„ä¼°ç­–ç•¥')
    parser.add_argument('--eval_goal', type=str, default='5.0,-0.5',
                       help='è¯„ä¼°ç›®æ ‡ä½ç½®')
    parser.add_argument('--eval_max_steps', type=int, default=1024,
                   help='è¯„ä¼°æ—¶çš„æœ€å¤§æ­¥æ•°')
    
    args = parser.parse_args()
    
    try:
        # BCé¢„è®­ç»ƒ
        agent, best_val_loss = pretrain_from_demonstrations(
            demo_dir=args.demo_dir,
            model_save_path=args.save_path,
            num_epochs=args.epochs,
            batch_size=args.batch_size,
            learning_rate=args.lr,
            val_split=args.val_split
        )
        
        # å¯é€‰ï¼šè¯„ä¼°ç­–ç•¥
        if args.evaluate:
            from envs.clearpath_nav_env import ClearpathNavEnv
            
            eval_goal = tuple(map(float, args.eval_goal.split(',')))
            env = ClearpathNavEnv(goal_pos=eval_goal, max_steps=args.eval_max_steps)
            
            results = evaluate_bc_policy(agent, env, num_episodes=10)
            
            env.close()
        
        print("\nâœ… å…¨éƒ¨å®Œæˆ!")
        
    except KeyboardInterrupt:
        print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()